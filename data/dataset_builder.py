#!/usr/bin/env python3
"""
Dataset Builder - ê¹”ë”í•˜ê²Œ ìž¬êµ¬í˜„ëœ ë²„ì „

ì£¼ìš” ê¸°ëŠ¥:
- ì›ë³¸ ë°ì´í„° ë¡œë“œ ë° í•„í„°ë§
- í”¼ì²˜ ìƒì„±
- ë¼ë²¨ ìƒì„±
- ì„ íƒì  Z-score ì •ê·œí™”
- ìºì‹± ì§€ì›
"""

import os
import hashlib
import json
from pathlib import Path
from typing import Dict, Iterable, Optional, Literal
from datetime import date

import polars as pl

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ìž„í¬íŠ¸
from .load_silver import scan_ohlcv, filter_ohlcv, sample_tickers, SILVER_ROOT
from features.feature_sets import add_feature_set
from labelers.basic import future_return_labels


Market = Literal["KR", "US"]
NormalizationStats = Dict[str, Dict[str, float]]


def _quick_counts(lf: pl.LazyFrame, tag: str, verbose: bool = False):
    """ë°ì´í„° í†µê³„ ì¶œë ¥"""
    if not verbose:
        return
    
    try:
        stats = (
            lf.select([
                pl.len().alias("rows"),
                pl.n_unique("ticker").alias("n_tickers"),
                pl.col("date").min().alias("min_date"),
                pl.col("date").max().alias("max_date"),
            ])
            .collect()
        )
        
        row = stats.row(0)
        print(f"[{tag}] rows={row[0]:,}, tickers={row[1]:,}, {row[2]}~{row[3]}")
    except Exception as e:
        print(f"[{tag}] stats error: {e}")


def _dir_latest_mtime(path: Path) -> int:
    """ë””ë ‰í† ë¦¬ ë‚´ ìµœì‹  ìˆ˜ì • ì‹œê°„ ë°˜í™˜"""
    try:
        if not path.exists():
            return 0
        
        latest = 0
        for root, dirs, files in os.walk(path):
            for name in files:
                try:
                    fp = os.path.join(root, name)
                    mtime_ns = os.stat(fp).st_mtime_ns
                    if mtime_ns > latest:
                        latest = mtime_ns
                except OSError:
                    continue
        return latest
    except Exception:
        return 0


def _stable_list(value: Optional[Iterable]) -> Optional[list]:
    """ì•ˆì •ì ì¸ ë¦¬ìŠ¤íŠ¸ ë³€í™˜"""
    if value is None:
        return None
    try:
        return sorted(list(value))
    except Exception:
        return list(value)


def _make_build_cache_key(
    years: Optional[Iterable[int]] = None,
    market: Optional[str] = None,
    exchanges: Optional[Iterable[str]] = None,
    tickers: Optional[Iterable[str]] = None,
    max_tickers: Optional[int] = None,
    start: Optional[date] = None,
    end: Optional[date] = None,
    feature_set: str = "v1",
    label_horizon: int = 5,
    label_task: str = "regression",
    label_thresh: float = 0.05,
    select_cols: Optional[Iterable[str]] = None,
    drop_na_rows: bool = True,
    normalize_features: bool = True,
    cache_invalidate_on: str = "silver_mtime",
) -> str:
    """ìºì‹œ í‚¤ ìƒì„±"""
    params = {
        "v": "build_dataset_v3_clean",
        "years": _stable_list(years),
        "market": market,
        "exchanges": _stable_list(exchanges),
        "tickers": _stable_list([t.upper() for t in tickers] if tickers else None),
        "max_tickers": max_tickers,
        "start": start.isoformat() if start else None,
        "end": end.isoformat() if end else None,
        "feature_set": feature_set,
        "label_horizon": label_horizon,
        "label_task": label_task,
        "label_thresh": label_thresh,
        "select_cols": _stable_list(select_cols),
        "drop_na_rows": drop_na_rows,
        "normalize_features": normalize_features,
        "invalidate": cache_invalidate_on,
    }
    
    if cache_invalidate_on == "silver_mtime":
        params["silver_latest_mtime_ns"] = _dir_latest_mtime(Path(SILVER_ROOT))
    
    blob = json.dumps(params, sort_keys=True, separators=(",", ":")).encode()
    return hashlib.sha256(blob).hexdigest()


def build_dataset(
    years: Optional[Iterable[int]] = None,
    market: Optional[Market] = None,
    exchanges: Optional[Iterable[str]] = None,
    tickers: Optional[Iterable[str]] = None,
    max_tickers: Optional[int] = None,
    start: Optional[date] = None,
    end: Optional[date] = None,
    feature_set: str = "v1",
    label_horizon: int = 5,
    label_task: str = "regression",   # or "classification"
    label_thresh: float = 0.05,
    select_cols: Optional[Iterable[str]] = None,
    drop_na_rows: bool = True,
    verbose: bool = False,
    *,
    use_cache: bool = True,
    cache_dir: str | Path = "data/cache/datasets",
    force_recompute: bool = False,
    cache_invalidate_on: Literal["never", "silver_mtime"] = "silver_mtime",
    normalize_features: bool = True,  # Z-score ì •ê·œí™” ì ìš© ì—¬ë¶€
    normalization_stats: Optional[NormalizationStats] = None,
    return_normalization_stats: bool = False,
) -> pl.DataFrame | tuple[pl.DataFrame, NormalizationStats]:
    """
    ë°ì´í„°ì…‹ ë¹Œë“œ í•¨ìˆ˜
    
    Args:
        years: ì—°ë„ ë¦¬ìŠ¤íŠ¸
        market: ì‹œìž¥ ì½”ë“œ (KR, US)
        exchanges: ê±°ëž˜ì†Œ ë¦¬ìŠ¤íŠ¸
        tickers: í‹°ì»¤ ë¦¬ìŠ¤íŠ¸
        max_tickers: ìµœëŒ€ í‹°ì»¤ ìˆ˜
        start: ì‹œìž‘ ë‚ ì§œ
        end: ì¢…ë£Œ ë‚ ì§œ
        feature_set: í”¼ì²˜ ì„¸íŠ¸ (v1, v2, v3)
        label_horizon: ë¼ë²¨ ì˜ˆì¸¡ ê¸°ê°„
        label_task: ë¼ë²¨ íƒœìŠ¤í¬ (regression, classification)
        label_thresh: ë¶„ë¥˜ ìž„ê³„ê°’
        select_cols: ì„ íƒí•  ì»¬ëŸ¼ë“¤
        drop_na_rows: NaN í–‰ ì œê±° ì—¬ë¶€
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
        use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
        cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬
        force_recompute: ê°•ì œ ìž¬ê³„ì‚° ì—¬ë¶€
        cache_invalidate_on: ìºì‹œ ë¬´íš¨í™” ì¡°ê±´
        normalize_features: Z-score ì •ê·œí™” ì ìš© ì—¬ë¶€
        normalization_stats: ì™¸ë¶€ì—ì„œ ì£¼ìž…ëœ ì •ê·œí™” í†µê³„ (mean/std). ì œê³µ ì‹œ í•´ë‹¹ ê°’ì„ ì‚¬ìš©
        return_normalization_stats: Trueì¼ ë•Œ (DataFrame, stats) íŠœí”Œ ë°˜í™˜
    
    Returns:
        ë¹Œë“œëœ ë°ì´í„°í”„ë ˆìž„
    """
    if verbose:
        print(f"Building dataset: {market}, years={years}, normalize={normalize_features}")
    
    normalization_stats_used: NormalizationStats | None = None

    # 1. ìºì‹œ í™•ì¸
    cache_path: Path | None = None
    if use_cache and not force_recompute:
        key = _make_build_cache_key(
            years=years,
            market=market,
            exchanges=exchanges,
            tickers=tickers,
            max_tickers=max_tickers,
            start=start,
            end=end,
            feature_set=feature_set,
            label_horizon=label_horizon,
            label_task=label_task,
            label_thresh=label_thresh,
            select_cols=select_cols,
            drop_na_rows=drop_na_rows,
            normalize_features=normalize_features,
            cache_invalidate_on=cache_invalidate_on,
        )
        cache_dir_path = Path(cache_dir)
        cache_dir_path.mkdir(parents=True, exist_ok=True)
        cache_path = cache_dir_path / f"{key[:16]}.parquet"
        
        if cache_path.exists():
            if verbose:
                print(f"[cache] hit: {cache_path}")
            return pl.read_parquet(str(cache_path))
    
    # 2. ë°ì´í„° ìŠ¤ìº”
    if verbose:
        print("ðŸ“Š Scanning OHLCV data...")
    lf = scan_ohlcv(market=market, tickers=tickers, years=years, exchanges=exchanges)
    _quick_counts(lf, "scan", verbose)
    
    # 3. ë°ì´í„° í•„í„°ë§
    if verbose:
        print("ðŸ” Filtering data...")
    lf = filter_ohlcv(
        lf,
        market=market,
        exchanges=exchanges,
        tickers=tickers,
        years=years,
        start=start,
        end=end,
        scan_market=market,
        scan_years=years,
        sort_result=False,
    )
    _quick_counts(lf, "filter", verbose)
    
    # 4. í‹°ì»¤ ìƒ˜í”Œë§
    if max_tickers:
        if verbose:
            print(f"ðŸŽ² Sampling max {max_tickers} tickers...")
        lf = sample_tickers(lf, max_tickers=max_tickers)
        _quick_counts(lf, "sample", verbose)
    
    # 5. ì •ë ¬ (ìœˆë„ìš° í•¨ìˆ˜ìš©)
    if verbose:
        print("ðŸ“ˆ Sorting by ticker and date...")
    lf = lf.sort(["ticker", "date"])
    
    # 6. í”¼ì²˜ ì¶”ê°€
    if verbose:
        print(f"âš™ï¸ Adding {feature_set} features...")
    lf = add_feature_set(lf, feature_set=feature_set)
    _quick_counts(lf, "features", verbose)
    
    # 7. ë¼ë²¨ ì¶”ê°€
    if verbose:
        print(f"ðŸ·ï¸ Adding labels (horizon={label_horizon}, task={label_task})...")
    lf = future_return_labels(lf, horizon=label_horizon, task=label_task, thresh=label_thresh)
    _quick_counts(lf, "labels", verbose)
    
    # 8. í”¼ì²˜ ì •ê·œí™” (ì„ íƒì )
    if normalize_features:
        if verbose:
            print("ðŸ”„ Applying Z-score normalization...")
        
        # ì •ê·œí™”í•  ì»¬ëŸ¼ ì‹ë³„
        schema_names = lf.collect_schema().names()
        base_cols = {
            "date", "ticker", "market", "exchange", "currency", "year",
            "open", "high", "low", "close", "adj_close", "volume", "turnover"
        }
        
        feature_cols = [
            c for c in schema_names 
            if c not in base_cols 
            and not c.startswith("label_") 
            and not c.startswith("futret_")
            and lf.collect_schema()[c] in (pl.Float32, pl.Float64, pl.Int32, pl.Int64, pl.UInt32, pl.UInt64)
        ]
        
        if feature_cols:
            provided_stats = normalization_stats or {}
            stats_for_use: NormalizationStats = {}
            newly_computed: NormalizationStats = {}

            def _sanitize_stats(mean_val: float, std_val: float) -> Dict[str, float]:
                std_val = float(std_val)
                if abs(std_val) < 1e-12:
                    std_val = 1.0
                return {"mean": float(mean_val), "std": std_val}

            missing_cols = [
                col for col in feature_cols
                if col not in provided_stats or abs(float(provided_stats[col].get("std", 0.0))) < 1e-12
            ]

            if missing_cols:
                stats_exprs = []
                for col in missing_cols:
                    stats_exprs.append(pl.col(col).mean().alias(f"{col}__mean"))
                    stats_exprs.append(pl.col(col).std().alias(f"{col}__std"))
                stats_frame = lf.select(stats_exprs).collect(streaming=False)

                for col in missing_cols:
                    mean_val = stats_frame[f"{col}__mean"][0]
                    std_val = stats_frame[f"{col}__std"][0]
                    stats_for_use[col] = _sanitize_stats(mean_val, std_val)
                newly_computed = {col: stats_for_use[col] for col in missing_cols}

            # ì£¼ìž…ëœ í†µê³„ í¬í•¨ (sanitize)
            for col in feature_cols:
                if col in stats_for_use:
                    continue
                if col in provided_stats:
                    stats_for_use[col] = _sanitize_stats(
                        provided_stats[col].get("mean", 0.0),
                        provided_stats[col].get("std", 1.0),
                    )

            # ëˆ„ë½ëœ ì»¬ëŸ¼ì€ í‰ê·  0, í‘œì¤€íŽ¸ì°¨ 1ë¡œ fallback
            for col in feature_cols:
                if col not in stats_for_use:
                    stats_for_use[col] = {"mean": 0.0, "std": 1.0}

            normalization_stats_used = {col: dict(stats_for_use[col]) for col in feature_cols}

            normalize_exprs = [
                ((pl.col(col) - stats_for_use[col]["mean"]) / stats_for_use[col]["std"]).alias(col)
                for col in feature_cols
            ]

            keep_cols = [c for c in schema_names if c not in feature_cols]
            lf = lf.with_columns(normalize_exprs).select(keep_cols + feature_cols)

            if verbose:
                computed_cnt = len(newly_computed)
                reused_cnt = len(feature_cols) - computed_cnt
                print(f"[normalize] Applied Z-score to {len(feature_cols)} features "
                      f"(computed={computed_cnt}, reused={reused_cnt})")
        else:
            if verbose:
                print("[normalize] No features to normalize")
    else:
        if verbose:
            print("[normalize] Skipping Z-score normalization")
    
    _quick_counts(lf, "normalized", verbose)
    
    # 9. NaN ì²˜ë¦¬
    if drop_na_rows:
        if verbose:
            print("ðŸ§¹ Dropping NaN rows...")
        
        # í”¼ì²˜ì™€ ë¼ë²¨ ì»¬ëŸ¼ì—ì„œ NaN ì œê±°
        schema_names = lf.collect_schema().names()
        feat_cols = [
            c for c in schema_names 
            if c not in ["date", "ticker", "market", "exchange", "open", "high", "low", "close", "adj_close", "volume", "turnover", "year"]
        ]
        
        if feat_cols:
            lf = lf.drop_nulls(feat_cols)
            _quick_counts(lf, "drop_na", verbose)
    
    # 10. ì»¬ëŸ¼ ì„ íƒ
    if select_cols:
        if verbose:
            print(f"ðŸ“‹ Selecting {len(select_cols)} columns...")
        lf = lf.select(list(select_cols))
    
    # 11. ë°ì´í„°í”„ë ˆìž„ ìˆ˜ì§‘
    if verbose:
        print("ðŸ’¾ Collecting final dataframe...")
    df = lf.collect(streaming=False)
    
    # 12. ìºì‹œ ì €ìž¥
    if use_cache and cache_path:
        try:
            df.write_parquet(str(cache_path))
            if verbose:
                print(f"[cache] saved: {cache_path}")
        except Exception as e:
            if verbose:
                print(f"[cache] save failed: {e}")
    
    if verbose:
        print(f"âœ… Dataset built: {len(df):,} rows Ã— {len(df.columns)} columns")

    if return_normalization_stats:
        return df, (normalization_stats_used or {})
    
    return df
