# models/M001_DirectionClassifier.py
"""
ë°©í–¥ ë¶„ë¥˜ ëª¨ë¸ (LightGBM ê¸°ë°˜ ì´ì§„ ë¶„ë¥˜ê¸°)

ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œë¥¼ ê³ ë ¤í•˜ì—¬ ì„ ë³„ëœ featureë“¤ì„ ì‚¬ìš©í•˜ì—¬
ìƒìŠ¹/í•˜ë½ ë°©í–¥ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.
"""

import os
import pickle
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

import polars as pl
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score,
    accuracy_score, precision_score, recall_score, f1_score
)
import lightgbm as lgb
import matplotlib.pyplot as plt
import seaborn as sns

from data.dataset_builder import build_dataset

# === Feature Selection (ë‹¤ì¤‘ê³µì„ ì„± ê³ ë ¤) ===
SELECTED_FEATURES = [
    # ìˆ˜ìµë¥  ê·¸ë£¹ (roc10 ì„ íƒ - ê°€ì¥ ë†’ì€ ì¤‘ìš”ë„)
    'roc10',

    # CCI ê·¸ë£¹ (cci20 ì„ íƒ - ë‘ ë²ˆì§¸ë¡œ ë†’ì€ ì¤‘ìš”ë„)
    'cci20',

    # Williams %R (willr14 ì„ íƒ - ì„¸ ë²ˆì§¸ë¡œ ë†’ì€ ì¤‘ìš”ë„)
    'willr14',

    # OBV ê·¸ë£¹ (obv ì„ íƒ - On Balance Volume, ë„¤ ë²ˆì§¸ ì¤‘ìš”ë„)
    'obv',

    # ATR ê·¸ë£¹ (atr14 ì„ íƒ - ë‹¤ì„¯ ë²ˆì§¸ ì¤‘ìš”ë„)
    'atr14',

    # MACD ê·¸ë£¹ (macd_hist ì„ íƒ - íˆìŠ¤í† ê·¸ë¨, ì—¬ì„¯ ë²ˆì§¸ ì¤‘ìš”ë„)
    'macd_hist',

    # RSI ê·¸ë£¹ (rsi14 ì„ íƒ - ì¼ê³± ë²ˆì§¸ ì¤‘ìš”ë„)
    'rsi14',

    # MFI ê·¸ë£¹ (mfi14 ì„ íƒ - ì—¬ëŸ ë²ˆì§¸ ì¤‘ìš”ë„)
    'mfi14',

    # Donchian ì±„ë„ (pos_in_don20 ì„ íƒ - ì•„í™‰ ë²ˆì§¸ ì¤‘ìš”ë„)
    'pos_in_don20',

    # ë³€ë™ì„± ê·¸ë£¹ (parkinson20 ì„ íƒ - Parkinson ë³€ë™ì„±, ì—´ ë²ˆì§¸ ì¤‘ìš”ë„)
    'parkinson20',

    # ìŠ¤í† ìºìŠ¤í‹± ê·¸ë£¹ (stochd14 ì„ íƒ - smoothed ë²„ì „)
    'stochd14',

    # ê±°ë˜ëŸ‰ ê·¸ë£¹ (vol_z20 ì„ íƒ - í‘œì¤€í™”ëœ ë³€ë™ì„±)
    'vol_z20',

    # VWAP ê·¸ë£¹ (vwap20 ì„ íƒ - ë” ê¸´ ê¸°ê°„)
    'vwap20'
]

class DirectionClassifierLGBM:
    """
    LightGBM ê¸°ë°˜ ë°©í–¥ ë¶„ë¥˜ ëª¨ë¸

    Features: ë‹¤ì¤‘ê³µì„ ì„± ê³ ë ¤í•˜ì—¬ ì„ ë³„ëœ 16ê°œ feature
    Target: +1 (ìƒìŠ¹), -1 (í•˜ë½) ì´ì§„ ë¶„ë¥˜
    """

    def __init__(self,
                 model_params: Optional[Dict] = None,
                 feature_list: Optional[List[str]] = None):
        """
        Args:
            model_params: LightGBM ëª¨ë¸ íŒŒë¼ë¯¸í„°
            feature_list: ì‚¬ìš©í•  feature ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: SELECTED_FEATURES)
        """
        self.feature_list = feature_list or SELECTED_FEATURES
        self.model_params = model_params or self._get_default_params()
        self.model = None
        self.feature_importance = None
        self.training_metrics = {}

    def _get_default_params(self) -> Dict:
        """ê¸°ë³¸ LightGBM íŒŒë¼ë¯¸í„°"""
        return {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1,
            'random_state': 42,
            'n_estimators': 100,
            'max_depth': 6,
            'min_child_samples': 20,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1
        }

    def load_data(self,
                  market: str = "KR",
                  years: List[int] = [2018, 2019, 2020],
                  max_tickers: int = 100,
                  feature_set: str = "v2",
                  label_horizon: int = 1,
                  label_thresh: float = 0.05) -> Tuple[pd.DataFrame, pd.Series]:
        """
        í•™ìŠµ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬

        Args:
            market: ì‹œì¥ ì½”ë“œ
            years: í•™ìŠµ ì—°ë„
            max_tickers: ìµœëŒ€ í‹°ì»¤ ìˆ˜
            feature_set: feature set
            label_horizon: ë¼ë²¨ horizon
            label_thresh: ë¼ë²¨ threshold

        Returns:
            X: feature ë°ì´í„°í”„ë ˆì„
            y: target ì‹œë¦¬ì¦ˆ (0: ìƒìŠ¹, 1: í•˜ë½)
        """
        print(f"ğŸ“Š Loading data for {market} market, years {years}...")

        # ë°ì´í„° ë¡œë“œ
        df = build_dataset(
            years=years,
            market=market,
            exchanges=None,
            tickers=None,
            max_tickers=max_tickers,
            start=None,
            end=None,
            feature_set=feature_set,
            label_horizon=label_horizon,
            label_task="classification",
            label_thresh=label_thresh,
            select_cols=None,
            drop_na_rows=True,
            verbose=False,
        )

        print(f"âœ… Loaded {len(df)} samples, {len(df.columns)} columns")

        # ì´ë²¤íŠ¸ ë°ì´í„°ë§Œ í•„í„°ë§ (label_1d_clsê°€ 0ì´ ì•„ë‹Œ ê²½ìš°)
        event_df = df.filter(pl.col("label_1d_cls") != 0)
        print(f"âœ… Filtered to {len(event_df)} directional events")

        if len(event_df) < 1000:
            print(f"âš ï¸ Warning: Only {len(event_df)} samples available. Consider using more data.")

        # Featureì™€ Target ë¶„ë¦¬
        available_features = [f for f in self.feature_list if f in event_df.columns]
        missing_features = [f for f in self.feature_list if f not in event_df.columns]

        if missing_features:
            print(f"âš ï¸ Missing features: {missing_features}")
            print(f"ğŸ“Š Using {len(available_features)} available features: {available_features}")

        # pandasë¡œ ë³€í™˜
        feature_df = event_df.select(available_features).to_pandas()
        target_series = event_df.select("label_1d_cls").to_pandas()["label_1d_cls"]

        # Target ë³€í™˜: -1, 1 -> 0, 1
        y = ((target_series + 1) // 2).astype(int)

        print(f"ğŸ¯ Target distribution: {y.value_counts().to_dict()}")
        print(f"ğŸ“Š Features shape: {feature_df.shape}")

        return feature_df, y

    def train(self,
              X: pd.DataFrame,
              y: pd.Series,
              test_size: float = 0.2,
              use_cv: bool = True,
              cv_folds: int = 5,
              validation_years: Optional[List[int]] = [2021]) -> Dict:
        """
        ëª¨ë¸ í•™ìŠµ (êµì°¨ê²€ì¦ + Validation í¬í•¨)

        Args:
            X: feature ë°ì´í„°
            y: target ë°ì´í„°
            test_size: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¹„ìœ¨
            use_cv: êµì°¨ê²€ì¦ ì‚¬ìš© ì—¬ë¶€
            cv_folds: êµì°¨ê²€ì¦ fold ìˆ˜
            validation_years: validationìš© ë°ì´í„° ì—°ë„ (ê¸°ë³¸ê°’: [2021])

        Returns:
            í•™ìŠµ ê²°ê³¼ ë©”íŠ¸ë¦­
        """
        print("ğŸš€ Training Direction Classifier...")

        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )

        print(f"ğŸ“Š Train set: {X_train.shape}, Test set: {X_test.shape}")

        # Validation ë°ì´í„° ë¡œë“œ (2021ë…„)
        validation_metrics = None
        if validation_years:
            print(f"ğŸ“Š Loading validation data for years: {validation_years}")
            try:
                X_val, y_val = self.load_data(
                    market="KR",
                    years=validation_years,
                    max_tickers=100,
                    feature_set="v2",
                    label_horizon=1,
                    label_thresh=0.05
                )
                print(f"ğŸ“Š Validation set: {X_val.shape}")

                # Validation ë°ì´í„°ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” featureë§Œ ì„ íƒ
                common_features = [f for f in self.feature_list if f in X_val.columns]
                X_val = X_val[common_features]
                X_train = X_train[common_features]
                X_test = X_test[common_features]

                print(f"ğŸ“Š Using {len(common_features)} common features for validation")

            except Exception as e:
                print(f"âš ï¸ Warning: Could not load validation data: {e}")
                X_val, y_val = None, None
        else:
            X_val, y_val = None, None

        # LightGBM ë°ì´í„°ì…‹ ìƒì„±
        train_data = lgb.Dataset(X_train, label=y_train)
        test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

        valid_sets = [train_data, test_data]
        valid_names = ['train', 'valid']

        if X_val is not None and y_val is not None:
            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
            valid_sets.append(val_data)
            valid_names.append('validation')

        # ëª¨ë¸ í•™ìŠµ
        self.model = lgb.train(
            self.model_params,
            train_data,
            valid_sets=valid_sets,
            valid_names=valid_names,
            callbacks=[
                lgb.early_stopping(stopping_rounds=20),
                lgb.log_evaluation(period=10)
            ]
        )

        # Train/Test ì˜ˆì¸¡ ë° ë©”íŠ¸ë¦­
        y_pred_proba_test = self.model.predict(X_test)
        y_pred_test = (y_pred_proba_test > 0.5).astype(int)

        # ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred_test),
            'precision': precision_score(y_test, y_pred_test, zero_division=0),
            'recall': recall_score(y_test, y_pred_test, zero_division=0),
            'f1_score': f1_score(y_test, y_pred_test, zero_division=0),
            'roc_auc': roc_auc_score(y_test, y_pred_proba_test),
            'confusion_matrix': confusion_matrix(y_test, y_pred_test),
            'classification_report': classification_report(y_test, y_pred_test, zero_division=0)
        }

        # Validation í‰ê°€ (2021ë…„ ë°ì´í„°)
        if X_val is not None and y_val is not None:
            y_pred_proba_val = self.model.predict(X_val)
            y_pred_val = (y_pred_proba_val > 0.5).astype(int)

            validation_metrics = {
                'val_accuracy': accuracy_score(y_val, y_pred_val),
                'val_precision': precision_score(y_val, y_pred_val, zero_division=0),
                'val_recall': recall_score(y_val, y_pred_val, zero_division=0),
                'val_f1_score': f1_score(y_val, y_pred_val, zero_division=0),
                'val_roc_auc': roc_auc_score(y_val, y_pred_proba_val),
                'val_confusion_matrix': confusion_matrix(y_val, y_pred_val),
                'val_classification_report': classification_report(y_val, y_pred_val, zero_division=0)
            }

            metrics.update(validation_metrics)

        # êµì°¨ê²€ì¦ (ì˜µì…˜)
        if use_cv:
            cv_scores = cross_val_score(
                lgb.LGBMClassifier(**self.model_params),
                X_train, y_train, cv=cv_folds, scoring='accuracy'
            )
            metrics['cv_accuracy_mean'] = cv_scores.mean()
            metrics['cv_accuracy_std'] = cv_scores.std()

        # Feature Importance ì €ì¥
        self.feature_importance = pd.DataFrame({
            'feature': X_train.columns,
            'importance': self.model.feature_importance(importance_type='gain')
        }).sort_values('importance', ascending=False)

        self.training_metrics = metrics

        print("âœ… Training completed!")
        print("\nğŸ“Š Test Performance:")
        print(f"  ğŸ¯ Accuracy: {metrics['accuracy']:.4f}")
        print(f"  ğŸ¯ ROC-AUC: {metrics['roc_auc']:.4f}")
        print(f"  ğŸ¯ Precision: {metrics['precision']:.4f}")
        print(f"  ğŸ¯ Recall: {metrics['recall']:.4f}")
        print(f"  ğŸ¯ F1-Score: {metrics['f1_score']:.4f}")

        if validation_metrics:
            print("\nğŸ“Š Validation Performance (2021):")
            print(f"  ğŸ¯ Accuracy: {validation_metrics['val_accuracy']:.4f}")
            print(f"  ğŸ¯ ROC-AUC: {validation_metrics['val_roc_auc']:.4f}")
            print(f"  ğŸ¯ Precision: {validation_metrics['val_precision']:.4f}")
            print(f"  ğŸ¯ Recall: {validation_metrics['val_recall']:.4f}")
            print(f"  ğŸ¯ F1-Score: {validation_metrics['val_f1_score']:.4f}")

        if use_cv:
            print("\nğŸ“Š Cross-Validation:")
            print(f"  ğŸ¯ CV Accuracy: {metrics['cv_accuracy_mean']:.4f} Â± {metrics['cv_accuracy_std']:.4f}")

        return metrics

    def predict(self, X: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """
        ì˜ˆì¸¡ ìˆ˜í–‰

        Args:
            X: feature ë°ì´í„°

        Returns:
            y_pred: ì˜ˆì¸¡ í´ë˜ìŠ¤ (0, 1)
            y_pred_proba: ì˜ˆì¸¡ í™•ë¥ 
        """
        if self.model is None:
            raise ValueError("Model not trained yet. Call train() first.")

        y_pred_proba = self.model.predict(X)
        y_pred = (y_pred_proba > 0.5).astype(int)

        return y_pred, y_pred_proba

    def evaluate(self, X: pd.DataFrame, y: pd.Series) -> Dict:
        """
        ëª¨ë¸ í‰ê°€

        Args:
            X: feature ë°ì´í„°
            y: ì‹¤ì œ target

        Returns:
            í‰ê°€ ë©”íŠ¸ë¦­
        """
        y_pred, y_pred_proba = self.predict(X)

        return {
            'accuracy': accuracy_score(y, y_pred),
            'precision': precision_score(y, y_pred, zero_division=0),
            'recall': recall_score(y, y_pred, zero_division=0),
            'f1_score': f1_score(y, y_pred, zero_division=0),
            'roc_auc': roc_auc_score(y, y_pred_proba),
            'confusion_matrix': confusion_matrix(y, y_pred),
            'classification_report': classification_report(y, y_pred, zero_division=0)
        }

    def plot_feature_importance(self, save_path: Optional[str] = None, top_n: int = 20):
        """Feature Importance ì‹œê°í™”"""
        if self.feature_importance is None:
            raise ValueError("Model not trained yet or no feature importance available.")

        plt.figure(figsize=(12, 8))

        top_features = self.feature_importance.head(top_n)
        bars = plt.barh(range(len(top_features)), top_features['importance'])
        plt.yticks(range(len(top_features)), top_features['feature'])
        plt.xlabel('Feature Importance (Gain)')
        plt.title(f'Direction Classifier - Top {top_n} Feature Importance', fontweight='bold')
        plt.grid(True, alpha=0.3, axis='x')

        # ê°’ ë ˆì´ë¸” ì¶”ê°€
        for i, v in enumerate(top_features['importance']):
            plt.text(v + max(top_features['importance']) * 0.01, i,
                    '.3f', va='center', fontsize=9)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=160, bbox_inches='tight')
            print(f"ğŸ’¾ Feature importance plot saved to {save_path}")

        plt.show()

    def plot_confusion_matrix(self, X: pd.DataFrame, y: pd.Series, save_path: Optional[str] = None):
        """Confusion Matrix ì‹œê°í™”"""
        y_pred, _ = self.predict(X)
        cm = confusion_matrix(y, y_pred)

        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['Down (-1)', 'Up (+1)'],
                   yticklabels=['Down (-1)', 'Up (+1)'])
        plt.title('Direction Classifier - Confusion Matrix', fontweight='bold')
        plt.ylabel('True Direction')
        plt.xlabel('Predicted Direction')
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=160, bbox_inches='tight')
            print(f"ğŸ’¾ Confusion matrix saved to {save_path}")

        plt.show()

    def save_model(self, filepath: str):
        """ëª¨ë¸ ì €ì¥"""
        if self.model is None:
            raise ValueError("No model to save. Train the model first.")

        # ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(filepath), exist_ok=True)

        # LightGBM ëª¨ë¸ ì €ì¥
        self.model.save_model(filepath)

        # ë©”íƒ€ë°ì´í„° ì €ì¥ (JSONìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì €ì¥)
        import json
        metadata = {
            'feature_list': self.feature_list,
            'model_params': self.model_params,
            'feature_importance': self.feature_importance.to_dict() if self.feature_importance is not None else None,
            'training_metrics': self.training_metrics
        }

        # JSONìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ì €ì¥ (pickle ë¬¸ì œ íšŒí”¼)
        json_metadata_path = filepath.replace('.txt', '_metadata.json')
        with open(json_metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2, default=str)

        # ê¸°ì¡´ pickle ë°©ì‹ë„ ìœ ì§€ (í˜¸í™˜ì„±)
        try:
            pickle_metadata_path = filepath.replace('.txt', '_metadata.pkl')
            with open(pickle_metadata_path, 'wb') as f:
                pickle.dump(metadata, f)
        except Exception as e:
            print(f"  [ê²½ê³ ] Pickle ì €ì¥ ì‹¤íŒ¨ (JSONìœ¼ë¡œ ì €ì¥ë¨): {e}")

        print(f"ğŸ’¾ Model saved to {filepath}")
        print(f"ğŸ’¾ Metadata saved to {json_metadata_path}")

    def load_model(self, filepath: str):
        """ëª¨ë¸ ë¡œë“œ"""
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"Model file not found: {filepath}")

        # LightGBM ëª¨ë¸ ë¡œë“œ
        self.model = lgb.Booster(model_file=filepath)

        # ë©”íƒ€ë°ì´í„° ë¡œë“œ (JSON ìš°ì„ , pickle fallback)
        import json
        metadata = None

        # 1. JSON íŒŒì¼ ì‹œë„ (ì•ˆì „í•œ ë°©ì‹)
        json_metadata_path = filepath.replace('.txt', '_metadata.json')
        if os.path.exists(json_metadata_path):
            try:
                with open(json_metadata_path, 'r') as f:
                    metadata = json.load(f)
                print(f"  âœ… JSON ë©”íƒ€ë°ì´í„° ë¡œë“œ ì„±ê³µ")
            except Exception as e:
                print(f"  [ê²½ê³ ] JSON ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

        # 2. Pickle íŒŒì¼ ì‹œë„ (ê¸°ì¡´ í˜¸í™˜ì„±)
        if metadata is None:
            pickle_metadata_path = filepath.replace('.txt', '_metadata.pkl')
            if os.path.exists(pickle_metadata_path):
                try:
                    with open(pickle_metadata_path, 'rb') as f:
                        metadata = pickle.load(f)
                    print(f"  âœ… Pickle ë©”íƒ€ë°ì´í„° ë¡œë“œ ì„±ê³µ")
                except Exception as e:
                    print(f"  [ê²½ê³ ] Pickle ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

        # 3. ë©”íƒ€ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì ìš©
        if metadata is not None:
            self.feature_list = metadata.get('feature_list', [])
            self.model_params = metadata.get('model_params', {})
            self.feature_importance = pd.DataFrame(metadata.get('feature_importance', {}))
            self.training_metrics = metadata.get('training_metrics', {})
        else:
            print(f"  [ê²½ê³ ] ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ì–´ ê¸°ë³¸ê°’ ì‚¬ìš©")

        print(f"ğŸ“‚ Model loaded from {filepath}")

def create_direction_classifier_model(market: str = "KR",
                                    years: List[int] = [2018, 2019, 2020],
                                    validation_years: Optional[List[int]] = [2021],
                                    save_model: bool = True,
                                    model_dir: str = "models/saved") -> DirectionClassifierLGBM:
    """
    ë°©í–¥ ë¶„ë¥˜ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ (êµì°¨ê²€ì¦ + Validation í¬í•¨)

    Args:
        market: ì‹œì¥ ì½”ë“œ
        years: í•™ìŠµ ì—°ë„
        validation_years: validationìš© ë°ì´í„° ì—°ë„ (ê¸°ë³¸ê°’: [2021])
        save_model: ëª¨ë¸ ì €ì¥ ì—¬ë¶€
        model_dir: ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬

    Returns:
        í•™ìŠµëœ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
    """
    print("ğŸ¯ Creating Direction Classifier Model...")
    print(f"ğŸ“Š Selected Features ({len(SELECTED_FEATURES)}):")
    for i, feature in enumerate(SELECTED_FEATURES, 1):
        print(f"  {i}. {feature}")
    print()

    # ëª¨ë¸ ìƒì„±
    model = DirectionClassifierLGBM()

    # ë°ì´í„° ë¡œë“œ
    X, y = model.load_data(market=market, years=years)

    # ëª¨ë¸ í•™ìŠµ (êµì°¨ê²€ì¦ + Validation í¬í•¨)
    metrics = model.train(X, y, validation_years=validation_years)

    # Classification Report ì¶œë ¥
    print("\nğŸ“Š Detailed Classification Report (Test Set):")
    print(metrics['classification_report'])

    if 'val_classification_report' in metrics:
        print("ğŸ“Š Detailed Classification Report (Validation Set - 2021):")
        print(metrics['val_classification_report'])

    # Confusion Matrix ì¶œë ¥
    print("ğŸ“Š Confusion Matrix (Test Set):")
    print(metrics['confusion_matrix'])

    if 'val_confusion_matrix' in metrics:
        print("ğŸ“Š Confusion Matrix (Validation Set - 2021):")
        print(metrics['val_confusion_matrix'])
    print()

    # Feature Importance ì¶œë ¥
    if model.feature_importance is not None:
        print("ğŸ¯ Top 10 Feature Importance:")
        top_10 = model.feature_importance.head(10)
        for i, row in top_10.iterrows():
            print(f"  {row['feature']}: {row['importance']:.4f}")
        print()

    # ëª¨ë¸ ì €ì¥
    if save_model:
        os.makedirs(model_dir, exist_ok=True)
        model_path = os.path.join(model_dir, f"direction_classifier_{market}_{'_'.join(map(str, years))}.txt")
        model.save_model(model_path)

    return model

if __name__ == "__main__":
    # ì˜ˆì‹œ ì‹¤í–‰ (êµì°¨ê²€ì¦ + 2021ë…„ Validation)
    model = create_direction_classifier_model(
        market="KR",
        years=[2018, 2019, 2020],
        validation_years=[2021],
        save_model=True
    )

    print("âœ… Direction Classifier Model created successfully!")
    print(f"ğŸ¯ Features used: {len(SELECTED_FEATURES)}")
    print(f"ğŸ† Test Accuracy: {model.training_metrics.get('accuracy', 'N/A')}")
    print(f"ğŸ† Test ROC-AUC: {model.training_metrics.get('roc_auc', 'N/A')}")

    if 'val_accuracy' in model.training_metrics:
        print(f"ğŸ† Validation Accuracy (2021): {model.training_metrics.get('val_accuracy', 'N/A')}")
        print(f"ğŸ† Validation ROC-AUC (2021): {model.training_metrics.get('val_roc_auc', 'N/A')}")

    if 'cv_accuracy_mean' in model.training_metrics:
        print(f"ğŸ† CV Accuracy: {model.training_metrics.get('cv_accuracy_mean', 'N/A'):.4f} Â± {model.training_metrics.get('cv_accuracy_std', 'N/A'):.4f}")

