#!/usr/bin/env python3
"""
Event Detector - M001 ì‹œë¦¬ì¦ˆ (TCN ê¸°ë°˜)

ì‹¤ì œ ë³€ë™í­ì„ ì¸¡ì •í•˜ì—¬ 5% ì´ìƒ ë“±ë½ ì´ë²¤íŠ¸ë¥¼ ê°ì§€í•˜ëŠ” ëª¨ë¸
TCN(Temporal Convolutional Network) ì•„í‚¤í…ì²˜ ì‚¬ìš©
Direction Classifierì™€ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ë” ì •í™•í•œ ì˜ˆì¸¡ ê°€ëŠ¥

íŠ¹ì§•:
- TCN ì•„í‚¤í…ì²˜ (L=60, dilation pyramid: 1,2,4,8,16)
- ì±„ë„ ìˆ˜: 160~256
- Dropout: 0.2
- FocalLoss ì ìš©
- Class-balanced sampler
- Temperature scaling
- ë ˆì§ ë³€ê²½ ê°ì§€ìš© ê³ ê¸‰ í”¼ì²˜ ì„¸íŠ¸ ë¬´ì¡°ê±´ ì‚¬ìš©

íŠ¹ì´ì‚¬í•­
2021ë…„ë„ ë°ì´í„°ë¡œ ê°€ë©´ "ë°•ì‚´ì´ ë‚˜ë²„ë¦¼"

"""

import os
import pickle
from pathlib import Path
from typing import Optional, List, Dict, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

import polars as pl
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use("Agg")
import seaborn as sns
from sklearn.model_selection import train_test_split, GroupShuffleSplit
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc, f1_score, precision_score, recall_score, balanced_accuracy_score
from sklearn.isotonic import IsotonicRegression
from sklearn.linear_model import LogisticRegression

# PyTorch ê´€ë ¨ ì„í¬íŠ¸
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from torch.optim import Adam
from torch.optim.lr_scheduler import CosineAnnealingLR

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
import sys
sys.path.append(str(Path(__file__).parent.parent))

from data.dataset_builder import build_dataset


# ============================================================================
# TCN ì•„í‚¤í…ì²˜ êµ¬í˜„
# ============================================================================

class Chomp1d(nn.Module):
    """
    Causal convolutionì„ ìœ„í•œ í—¬í¼ í´ë˜ìŠ¤
    ë¯¸ë˜ ì •ë³´ë¥¼ ë³´ì§€ ì•Šë„ë¡ í•˜ê¸° ìœ„í•´ ì¶œë ¥ì˜ ëë¶€ë¶„ì„ ì œê±°
    """
    def __init__(self, chomp_size):
        super(Chomp1d, self).__init__()
        self.chomp_size = chomp_size

    def forward(self, x):
        return x[:, :, :-self.chomp_size].contiguous()


class TemporalBlock(nn.Module):
    """
    TCNì˜ ê¸°ë³¸ Residual Block
    Dilated causal convolution + residual connection
    """
    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):
        super(TemporalBlock, self).__init__()

        # ì²« ë²ˆì§¸ dilated convolution
        self.conv1 = nn.utils.weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,
                                                   stride=stride, padding=padding, dilation=dilation))
        self.chomp1 = Chomp1d(padding)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)

        # ë‘ ë²ˆì§¸ dilated convolution
        self.conv2 = nn.utils.weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,
                                                   stride=stride, padding=padding, dilation=dilation))
        self.chomp2 = Chomp1d(padding)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)

        # Residual connectionì„ ìœ„í•œ 1x1 convolution (ì°¨ì› ë§ì¶”ê¸°ìš©)
        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,
                                self.conv2, self.chomp2, self.relu2, self.dropout2)
        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
        self.relu = nn.ReLU()
        self.init_weights()

    def init_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        self.conv1.weight.data.normal_(0, 0.01)
        self.conv2.weight.data.normal_(0, 0.01)
        if self.downsample is not None:
            self.downsample.weight.data.normal_(0, 0.01)

    def forward(self, x):
        out = self.net(x)
        res = x if self.downsample is None else self.downsample(x)
        return self.relu(out + res)


class TemporalConvNet(nn.Module):
    """
    Temporal Convolutional Network ë©”ì¸ ì•„í‚¤í…ì²˜
    ì—¬ëŸ¬ dilated convolution ë ˆì´ì–´ë¥¼ ìŒ“ì•„ ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ
    """
    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):
        """
        Args:
            num_inputs: ì…ë ¥ í”¼ì²˜ ìˆ˜
            num_channels: ê° ë ˆì´ì–´ì˜ ì±„ë„ ìˆ˜ ë¦¬ìŠ¤íŠ¸ [160, 192, 224, 256, 256]
            kernel_size: convolution ì»¤ë„ í¬ê¸°
            dropout: dropout ë¹„ìœ¨
        """
        super(TemporalConvNet, self).__init__()
        layers = []
        num_levels = len(num_channels)

        for i in range(num_levels):
            dilation_size = 2 ** i  # 1, 2, 4, 8, 16...
            in_channels = num_inputs if i == 0 else num_channels[i-1]
            out_channels = num_channels[i]

            layers += [TemporalBlock(in_channels, out_channels, kernel_size,
                                    stride=1, dilation=dilation_size,
                                    padding=(kernel_size-1) * dilation_size,
                                    dropout=dropout)]

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)


class FocalLoss(nn.Module):
    """
    Focal Loss êµ¬í˜„
    í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì†ì‹¤ í•¨ìˆ˜
    """
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

# ë³€ë™í­ ê°ì§€ìš© í”¼ì²˜ ì„¸íŠ¸
VOLATILITY_FEATURES = [
    # ê°€ê²© ë³€ë™ì„± ì§€í‘œ
    'atr14', 'atr5',           # Average True Range
    'parkinson20',         # Parkinson Volatility
    'vol_z20',              # Volume Z-score

    # ê°€ê²© ìœ„ì¹˜ ì§€í‘œ
    'rsi5',                    # RSI
    'stoch_spread',             # Stochastic %K-%D spread
    'willr14',                          # Williams %R

    # ì´ë™í‰ê·  ê´€ë ¨
    'ema5',           # Exponential Moving Average

    # ëª¨ë©˜í…€ ì§€í‘œ
    'roc5',           # Rate of Change
    'macd_hist', # MACD

    # ê±°ë˜ëŸ‰ ì§€í‘œ
    'vwap20',                  # VWAP
    'obv',
    'cmf20',
    'cci20',            # Volume indicators
]





# í†µí•© í”¼ì²˜ ì„¸íŠ¸ (ë³€ë™ì„± í”¼ì²˜ë§Œ ì‚¬ìš©)
ENHANCED_FEATURES = VOLATILITY_FEATURES


class TimeSeriesDataset(Dataset):
    """
    ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ìœ„í•œ PyTorch Dataset
    TCN ëª¨ë¸ ì…ë ¥ì„ ìœ„í•œ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
    """
    def __init__(self, df: pl.DataFrame, features: List[str], target_col: str,
                 sequence_length: int = 60, stride: int = 1):
        """
        Args:
            df: ì…ë ¥ ë°ì´í„°í”„ë ˆì„ (ì´ë²¤íŠ¸ ë¼ë²¨ì´ ì´ë¯¸ ê³„ì‚°ëœ ìƒíƒœ)
            features: ì‚¬ìš©í•  í”¼ì²˜ ë¦¬ìŠ¤íŠ¸
            target_col: íƒ€ê²Ÿ ì»¬ëŸ¼ëª…
            sequence_length: ì‹œí€€ìŠ¤ ê¸¸ì´ (L=60)
            stride: ì‹œí€€ìŠ¤ ìƒì„± ê°„ê²©
        """
        self.sequence_length = sequence_length
        self.stride = stride

        # íƒ€ê²Ÿ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸ (ì˜ˆì¸¡ ì‹œì—ëŠ” ì—†ì–´ë„ ë¨)
        if target_col and target_col not in df.columns:
            # ì˜ˆì¸¡ ì‹œì—ëŠ” íƒ€ê²Ÿ ì»¬ëŸ¼ì´ ì—†ì–´ë„ ì§„í–‰
            target_col = None

        if target_col is not None:
            # ê²°ì¸¡ì¹˜ê°€ ì—†ëŠ” ë°ì´í„°ë§Œ ì‚¬ìš©
            df_clean = df.filter(
                (~pl.col(target_col).is_null()) &
                (~pl.any_horizontal(pl.col(features).is_null()))
            )

            # ë°ì´í„°ë¥¼ pandasë¡œ ë³€í™˜
            self.data = df_clean.select(features).to_pandas().values.astype(np.float32)
            targets_series = df_clean.select(target_col).to_pandas().iloc[:, 0]

            # None/NaN ê°’ ì²˜ë¦¬
            targets_series = targets_series.fillna(0).astype(int)
            self.targets = targets_series.values.astype(np.int64)
        else:
            # ì˜ˆì¸¡ ì‹œì—ëŠ” íƒ€ê²Ÿ ì—†ì´ í”¼ì²˜ë§Œ ì‚¬ìš©
            df_clean = df.filter(
                ~pl.any_horizontal(pl.col(features).is_null())
            )
            self.data = df_clean.select(features).to_pandas().values.astype(np.float32)
            self.targets = np.zeros(len(self.data), dtype=np.int64)  # ë”ë¯¸ íƒ€ê²Ÿ

        # ì‹œí€€ìŠ¤ ìƒì„±ì„ ìœ„í•œ ì¸ë±ìŠ¤ ê³„ì‚°
        self.valid_indices = []
        for i in range(0, len(self.data) - sequence_length + 1, stride):
            self.valid_indices.append(i)

        print(f"  ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ ìƒì„±: {len(self.valid_indices)}ê°œ ì‹œí€€ìŠ¤")
        print(f"  ë°ì´í„° shape: {self.data.shape}, íƒ€ê²Ÿ ë¶„í¬: {np.bincount(self.targets)}")

    def __len__(self):
        return len(self.valid_indices)

    def __getitem__(self, idx):
        start_idx = self.valid_indices[idx]
        end_idx = start_idx + self.sequence_length

        # ì‹œí€€ìŠ¤ ë°ì´í„°: [sequence_length, num_features]
        x = self.data[start_idx:end_idx].T  # Conv1d ì…ë ¥ í˜•ì‹: [channels, length]
        y = self.targets[end_idx - 1]  # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ íƒ€ê²Ÿ

        return torch.FloatTensor(x), torch.LongTensor([y]).squeeze()


class EventDetectorTCN(nn.Module):
    """
    TCN ê¸°ë°˜ ì´ë²¤íŠ¸ ê°ì§€ê¸°
    ë ˆì§ ë³€ê²½ ê°ì§€ìš© ê³ ê¸‰ í”¼ì²˜ ì„¸íŠ¸ë¥¼ ë¬´ì¡°ê±´ ì‚¬ìš©
    """
    def __init__(self, num_features: int, sequence_length: int = 60,
                 num_channels: List[int] = None, dropout: float = 0.2):
        """
        Args:
            num_features: ì…ë ¥ í”¼ì²˜ ìˆ˜
            sequence_length: ì‹œí€€ìŠ¤ ê¸¸ì´
            num_channels: TCN ì±„ë„ ìˆ˜ ë¦¬ìŠ¤íŠ¸
            dropout: dropout ë¹„ìœ¨
        """
        super(EventDetectorTCN, self).__init__()

        # ê¸°ë³¸ ì±„ë„ ì„¤ì • (160~256)
        if num_channels is None:
            num_channels = [160, 192, 224, 256, 256]

        # TCN ë„¤íŠ¸ì›Œí¬
        self.tcn = TemporalConvNet(
            num_inputs=num_features,
            num_channels=num_channels,
            kernel_size=2,
            dropout=dropout
        )

        # ë¶„ë¥˜ í—¤ë“œ
        self.classifier = nn.Sequential(
            nn.Linear(num_channels[-1], 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, 2)  # Binary classification
        )

        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.apply(self._init_weights)

    def _init_weights(self, module):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Conv1d):
            nn.init.kaiming_normal_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)

    def forward(self, x):
        """
        Args:
            x: [batch_size, num_features, sequence_length]
        Returns:
            logits: [batch_size, 2]
        """
        # TCN ì ìš©
        tcn_out = self.tcn(x)  # [batch_size, num_channels[-1], output_length]

        # Global average pooling (ì‹œê°„ ì°¨ì› ì¶•ì†Œ)
        pooled = torch.mean(tcn_out, dim=2)  # [batch_size, num_channels[-1]]

        # ë¶„ë¥˜
        logits = self.classifier(pooled)
        return logits


class EventDetectorManager:
    """
    TCN ê¸°ë°˜ ì´ë²¤íŠ¸ ê°ì§€ê¸° ê´€ë¦¬ í´ë˜ìŠ¤
    ë ˆì§ ë³€ê²½ ê°ì§€ìš© ê³ ê¸‰ í”¼ì²˜ ì„¸íŠ¸ë¥¼ ë¬´ì¡°ê±´ ì‚¬ìš©

    íŠ¹ì§•:
    - TCN ì•„í‚¤í…ì²˜ (L=60, dilation pyramid: 1,2,4,8,16)
    - ì±„ë„ ìˆ˜: 160~256
    - Dropout: 0.2
    - FocalLoss ì ìš©
    - Class-balanced sampler
    - Temperature scaling
    """

    def __init__(self, threshold: float = 1.0, sequence_length: int = 60,
                 num_channels: List[int] = None, dropout: float = 0.2,
                 device: str = 'auto'):
        """
        ATR ê¸°ë°˜ ì •ê·œí™” ì´ë²¤íŠ¸ ê°ì§€ê¸° ì´ˆê¸°í™”

        Args:
            threshold: ATR ì •ê·œí™” ì´ë²¤íŠ¸ ê°ì§€ ì„ê³„ê°’ (ê¸°ë³¸ 1.0 = ATRì˜ 100%)
            sequence_length: TCN ì‹œí€€ìŠ¤ ê¸¸ì´ (L=60)
            num_channels: TCN ì±„ë„ ìˆ˜ ë¦¬ìŠ¤íŠ¸ [160, 192, 224, 256, 256]
            dropout: dropout ë¹„ìœ¨ (0.2)
            device: ì—°ì‚° ì¥ì¹˜ ('auto', 'cpu', 'cuda')
        """
        self.threshold = threshold
        self.sequence_length = sequence_length
        self.dropout = dropout

        # ë ˆì§ ë³€ê²½ ê°ì§€ìš© ê³ ê¸‰ í”¼ì²˜ ì„¸íŠ¸ ë¬´ì¡°ê±´ ì‚¬ìš©
        self.features = ENHANCED_FEATURES

        # TCN ì±„ë„ ì„¤ì •
        if num_channels is None:
            self.num_channels = [160, 192, 224, 256, 256]

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)

        # ëª¨ë¸ ë° í•™ìŠµ ê´€ë ¨ ë³€ìˆ˜ë“¤
        self.model = None
        self.scaler = StandardScaler()
        self.training_history = {}

        # FocalLossì™€ ê´€ë ¨ íŒŒë¼ë¯¸í„°ë“¤
        self.focal_loss_alpha = 1.0
        self.focal_loss_gamma = 2.0

        # Calibration ê´€ë ¨ ë³€ìˆ˜ë“¤
        self.probability_calibrator = None
        self.precision_threshold = None
        self.target_precision = 0.4  # ëª©í‘œ precision (40%)
        self.daily_top_k = 5  # ê¸°ë³¸ daily top-k (í•˜ë£¨ì— ìµœëŒ€ 5ê°œ ì´ë²¤íŠ¸)

        print(f"[EventDetectorManager] ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"  ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"  ì‹œí€€ìŠ¤ ê¸¸ì´: {self.sequence_length}")
        print(f"  í”¼ì²˜ ìˆ˜: {len(self.features)}")
        print(f"  TCN ì±„ë„: {self.num_channels}")



    def _create_class_balanced_sampler(self, targets: torch.Tensor) -> WeightedRandomSampler:
        """
        Class-balanced sampler ìƒì„±
        í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ìƒ˜í”ŒëŸ¬

        Args:
            targets: íƒ€ê²Ÿ ë¼ë²¨ í…ì„œ

        Returns:
            WeightedRandomSampler
        """
        class_counts = torch.bincount(targets)
        class_weights = 1.0 / class_counts.float()

        # ê° ìƒ˜í”Œì˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        sample_weights = class_weights[targets]

        # WeightedRandomSampler ìƒì„±
        sampler = WeightedRandomSampler(
            weights=sample_weights,
            num_samples=len(sample_weights),
            replacement=True
        )

        return sampler

    def _get_default_params(self, scale_pos_weight: float = 1.0) -> Dict[str, Any]:
        """LightGBM ê¸°ë³¸ íŒŒë¼ë¯¸í„°"""
        return {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'min_child_samples': 20,
            'scale_pos_weight': scale_pos_weight,  # í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬
            'random_state': 42,
            'verbose': -1
        }
    
    def _calculate_event_labels(self, df: pl.DataFrame) -> pl.DataFrame:
        """
        Intraday max/min ê¸°ë°˜ ì •ê·œí™” ì´ë²¤íŠ¸ ë¼ë²¨ ê³„ì‚°

        ATRì„ ìˆ˜ìµë¥  ë‹¨ìœ„ë¡œ ë³€í™˜í•˜ì—¬ ì´ë²¤íŠ¸ ê°ì§€:
        event = max(high/open-1, open/low-1) >= k * ATR%
        
        Args:
            df: ì…ë ¥ ë°ì´í„°í”„ë ˆì„
            
        Returns:
            ì´ë²¤íŠ¸ ë¼ë²¨ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
        """
        print(f"[Intraday range ê¸°ë°˜ ì´ë²¤íŠ¸ ë¼ë²¨ ê³„ì‚°] ì„ê³„ê°’: {self.threshold:.2f}")

        # ë‹¤ìŒë‚  intraday max/min ê¸°ë°˜ ìˆ˜ìµë¥  ê³„ì‚° (None ê°’ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
        # max(high_{t+1}/open_{t+1}-1, open_{t+1}/low_{t+1}-1)
        df_with_events = df.with_columns([
            pl.when(
                pl.col("open").shift(-1).is_null() |
                pl.col("high").shift(-1).is_null() |
                pl.col("low").shift(-1).is_null()
            )
              .then(None)
              .otherwise(
                  pl.max_horizontal([
                      (pl.col("high").shift(-1) / pl.col("open").shift(-1) - 1).abs(),
                      (pl.col("open").shift(-1) / pl.col("low").shift(-1) - 1).abs()
                  ])
              )
              .alias("next_day_intraday_range")
        ])

        # ATR14 ê²°ì¸¡ ì²˜ë¦¬ ë° ìˆ˜ìµë¥  ë‹¨ìœ„ ë³€í™˜
        df_with_events = df_with_events.with_columns([
            # ATR14 ê²°ì¸¡ ì²˜ë¦¬: ê²°ì¸¡ì‹œ ê¸°ë³¸ ë³€ë™ì„± ê°’ ì‚¬ìš© (2%)
            pl.when(
                pl.col("atr14").is_null() |
                (pl.col("atr14") == 0) |
                (pl.col("atr14") < 0) |
                pl.col("close").is_null() |
                (pl.col("close") == 0)
            )
              .then(0.05)
              .otherwise(pl.col("atr14") / pl.col("close"))
              .alias("atr_percent")
        ])

        # ì´ë²¤íŠ¸ ë¼ë²¨ ê³„ì‚°: intraday range >= k * ATR% (None ê°’ ì²˜ë¦¬)
        df_with_events = df_with_events.with_columns([
            pl.when(pl.col("next_day_intraday_range").is_null())
              .then(False)
              .otherwise(pl.col("next_day_intraday_range") >= self.threshold * pl.col("atr_percent"))
              .alias("big_move_event"),

            pl.when(pl.col("next_day_intraday_range").is_null())
              .then(False)
              .otherwise(pl.col("next_day_intraday_range") >= self.threshold * pl.col("atr_percent"))
              .alias("big_up_event"),

            pl.when(pl.col("next_day_intraday_range").is_null())
              .then(False)
              .otherwise(pl.col("next_day_intraday_range") >= self.threshold * pl.col("atr_percent"))
              .alias("big_down_event"),

            pl.when(pl.col("next_day_intraday_range").is_null())
              .then(False)
              .otherwise(pl.col("next_day_intraday_range") >= self.threshold * pl.col("atr_percent"))
              .alias("big_directional_event")
        ])

        # ë””ë²„ê¹…: ë¼ë²¨ ë¶„í¬ í™•ì¸
        label_stats = df_with_events.select([
            pl.col("big_move_event").sum().alias("events"),
            pl.len().alias("total")
        ]).row(0)

        print(f"  ì´ë²¤íŠ¸ ë¼ë²¨ ë¶„í¬: {label_stats[0]}/{label_stats[1]} ({label_stats[0]/label_stats[1]:.1%})")

        return df_with_events

    def _tune_threshold_with_cv(self, df: pl.DataFrame, k_values: List[float] = None,
                               target_col: str = "big_move_event") -> Dict[str, Any]:
        """
        ì—°ë„ë³„-í‹°ì»¤ë…ë¦½ ì‹œê³„ì—´ CVë¡œ k(ATR ë°°ìˆ˜) íŠœë‹
        
        Args:
            df: ì…ë ¥ ë°ì´í„°í”„ë ˆì„
            k_values: íŠœë‹í•  k ê°’ë“¤ (ê¸°ë³¸: [0.8, 1.0, 1.2, 1.5])
            target_col: íƒ€ê²Ÿ ì»¬ëŸ¼ëª…
            
        Returns:
            íŠœë‹ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if k_values is None:
            k_values = [0.8, 1.0, 1.2, 1.5]

        print(f"[k(ATR ë°°ìˆ˜) íŠœë‹] k âˆˆ {k_values}")

        # ì‹œê°„ ê¸°ë°˜ CVë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ (ì‹œì  leakage ë°©ì§€)
        df_sorted = df.sort("date")
        dates = df_sorted.select("date").to_pandas()['date'].values
        unique_dates = sorted(np.unique(dates))

        if len(unique_dates) < 2:
            print("  [ê²½ê³ ] CVë¥¼ ìœ„í•œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ k=1.0 ì‚¬ìš©")
            return {'optimal_k': 1.0, 'results': {}}

        # ì‹œê°„ ê¸°ë°˜ CV ìˆ˜í–‰ (Expanding window ë°©ì‹)
        cv_results = {}
        best_k = 1.0
        best_pr_auc = 0.0

        # CV fold ìˆ˜ ê²°ì • (ìµœëŒ€ 3-fold)
        n_folds = min(3, len(unique_dates) // 2)

        for k in k_values:
            pr_aucs = []
            print(f"  k = {k:.1f} í‰ê°€ ì¤‘...")

            for fold in range(n_folds):
                # Train: ì´ì „ ë°ì´í„°ë“¤
                train_end_idx = int(len(unique_dates) * (fold + 1) / n_folds)
                train_dates = unique_dates[:train_end_idx]

                # Test: ë‹¤ìŒ ë°ì´í„°ë“¤ (ì‹œê°„ ìˆœì„œ ìœ ì§€)
                test_start_idx = train_end_idx
                test_end_idx = int(len(unique_dates) * (fold + 2) / n_folds) if fold < n_folds - 1 else len(unique_dates)
                test_dates = unique_dates[test_start_idx:test_end_idx]

                if len(test_dates) == 0:
                    continue

                # ë‚ ì§œë¥¼ datetime.dateë¡œ ë³€í™˜í•˜ì—¬ í•„í„°ë§ (ë‹¤ì–‘í•œ í¬ë§· ì§€ì›)
                from datetime import date, datetime
                import pandas as pd

                def parse_date(d):
                    if isinstance(d, date):
                        return d
                    try:
                        # ISO í¬ë§· ë¬¸ìì—´ ì²˜ë¦¬
                        date_str = str(d).split('T')[0] if 'T' in str(d) else str(d).split()[0]
                        return date.fromisoformat(date_str)
                    except (ValueError, AttributeError):
                        # pandasë¥¼ ì‚¬ìš©í•œ ìœ ì—°í•œ íŒŒì‹±
                        try:
                            return pd.to_datetime(d).date()
                        except:
                            raise ValueError(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {d}")

                train_dates_dt = [parse_date(d) for d in train_dates]
                test_dates_dt = [parse_date(d) for d in test_dates]

                train_df = df_sorted.filter(pl.col("date").is_in(train_dates_dt))
                test_df = df_sorted.filter(pl.col("date").is_in(test_dates_dt))

                if len(train_df) == 0 or len(test_df) == 0:
                    continue

                # ì„ì‹œ thresholdë¡œ ì´ë²¤íŠ¸ ë¼ë²¨ ê³„ì‚°
                temp_detector = EventDetectorManager(threshold=k, sequence_length=self.sequence_length)
                train_with_labels = temp_detector._calculate_event_labels(train_df)
                test_with_labels = temp_detector._calculate_event_labels(test_df)

                # ê°„ë‹¨í•œ baseline ëª¨ë¸ë¡œ í‰ê°€ (ì—¬ê¸°ì„œëŠ” ì´ë²¤íŠ¸ ë°œìƒë¥  ê¸°ë°˜)
                train_event_rate = train_with_labels.select(target_col).mean().to_numpy()[0]
                test_events = test_with_labels.select(target_col).to_numpy().flatten()
                test_intraday_ranges = test_with_labels.select("next_day_intraday_range").to_numpy().flatten()

                if len(test_events) == 0 or test_events.sum() == 0:
                    pr_aucs.append(0.0)
                    continue

                # PR-AUC ê³„ì‚° (ê°„ë‹¨í•œ baseline)
                # ì´ë²¤íŠ¸ê°€ ë°œìƒí•œ ì¼€ì´ìŠ¤ë“¤ì˜ í‰ê·  intraday range vs non-event ì¼€ì´ìŠ¤ë“¤
                event_ranges = test_intraday_ranges[test_events == 1]
                non_event_ranges = test_intraday_ranges[test_events == 0]

                if len(event_ranges) == 0 or len(non_event_ranges) == 0:
                    pr_aucs.append(0.0)
                    continue

                # ê°„ë‹¨í•œ precision/recall ê³„ì‚°
                threshold = k * train_df.select("atr14").mean().to_numpy()[0] / train_df.select("close").mean().to_numpy()[0]
                pred_events = test_intraday_ranges >= threshold

                if pred_events.sum() == 0:
                    pr_aucs.append(0.0)
            else:
                    precision = (pred_events & test_events).sum() / pred_events.sum()
                    recall = (pred_events & test_events).sum() / test_events.sum()
                    pr_auc = precision * recall  # ê°„ë‹¨í•œ ê·¼ì‚¬
                    pr_aucs.append(pr_auc)

            avg_pr_auc = np.mean(pr_aucs) if pr_aucs else 0.0
            cv_results[k] = avg_pr_auc

            print(f"    k = {k:.1f}: PR-AUC = {avg_pr_auc:.4f}")

            if avg_pr_auc > best_pr_auc:
                best_pr_auc = avg_pr_auc
                best_k = k

        print(f"  ìµœì  k: {best_k:.1f} (PR-AUC: {best_pr_auc:.4f})")

        return {
            'optimal_k': best_k,
            'best_pr_auc': best_pr_auc,
            'results': cv_results
        }

    def _tune_threshold_with_precision(self, df: pl.DataFrame, target_precision: float = 0.4,
                                     target_col: str = "big_move_event") -> Dict[str, Any]:
        """
        Precision@k ê¸°ë°˜ ì„ê³„ê°’ íŠœë‹

        Args:
            df: ì…ë ¥ ë°ì´í„°í”„ë ˆì„
            target_precision: ëª©í‘œ precision (ì˜ˆ: 0.4 = 40%)
            target_col: íƒ€ê²Ÿ ì»¬ëŸ¼ëª…

        Returns:
            íŠœë‹ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print(f"[Precision@k ê¸°ë°˜ ì„ê³„ê°’ íŠœë‹] ëª©í‘œ precision: {target_precision:.1%}")

        # ë°ì´í„° ì¤€ë¹„
        pred_df = self._calculate_event_labels(df)
        available_features = [f for f in self.features if f in pred_df.columns]

        if not available_features:
            print("  [ê²½ê³ ] ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {'precision_threshold': 0.5, 'results': {}}

        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        pred_df = pred_df.filter(~pl.any_horizontal(pl.col(available_features).is_null()))

        if len(pred_df) == 0:
            print("  [ê²½ê³ ] ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {'precision_threshold': 0.5, 'results': {}}

        # ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ ìƒì„±
        pred_dataset = TimeSeriesDataset(
            df=pred_df,
            features=available_features,
            target_col=target_col,
            sequence_length=self.sequence_length,
            stride=1
        )

        pred_loader = DataLoader(pred_dataset, batch_size=64, shuffle=False, drop_last=False)

        # ëª¨ë¸ ì˜ˆì¸¡
        self.model.eval()
        all_probs = []
        all_targets = []

        with torch.no_grad():
            for batch_x, batch_y in pred_loader:
                batch_x = batch_x.to(self.device)
                outputs = self.model(batch_x)
                probs = torch.softmax(outputs, dim=1)[:, 1]

                all_probs.extend(probs.cpu().numpy())
                all_targets.extend(batch_y.cpu().numpy())

        y_true = np.array(all_targets)
        y_proba = np.array(all_probs)

        if len(y_true) == 0 or y_true.sum() == 0:
            print("  [ê²½ê³ ] ì´ë²¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {'precision_threshold': 0.5, 'results': {}}

        # Precision@k ê³¡ì„  ê³„ì‚°
        thresholds = np.linspace(0.01, 0.99, 99)
        precision_at_k = []

        for threshold in thresholds:
            y_pred = (y_proba >= threshold).astype(int)
            if y_pred.sum() > 0:
                precision = (y_true & y_pred).sum() / y_pred.sum()
                precision_at_k.append((threshold, precision))
            else:
                precision_at_k.append((threshold, 0.0))

        # ëª©í‘œ precisionì— ê°€ì¥ ê°€ê¹Œìš´ threshold ì°¾ê¸°
        precision_at_k = np.array(precision_at_k)
        precision_values = precision_at_k[:, 1]

        # ëª©í‘œ precision ì´ìƒì¸ ê²ƒë“¤ ì¤‘ ê°€ì¥ ë‚®ì€ threshold ì„ íƒ
        valid_indices = precision_values >= target_precision
        if valid_indices.any():
            # ëª©í‘œ precision ì´ìƒì¸ ê²ƒë“¤ ì¤‘ thresholdê°€ ê°€ì¥ ë‚®ì€ ê²ƒ ì„ íƒ
            valid_thresholds = precision_at_k[valid_indices]
            optimal_idx = np.argmin(valid_thresholds[:, 0])
            optimal_threshold = valid_thresholds[optimal_idx, 0]
            optimal_precision = valid_thresholds[optimal_idx, 1]
        else:
            # ëª©í‘œ precisionì„ ë‹¬ì„±í•˜ì§€ ëª»í•˜ë©´ ê°€ì¥ ë†’ì€ precisionì˜ threshold ì„ íƒ
            optimal_idx = np.argmax(precision_values)
            optimal_threshold = precision_at_k[optimal_idx, 0]
            optimal_precision = precision_at_k[optimal_idx, 1]

        print(f"  ìµœì  ì„ê³„ê°’: {optimal_threshold:.3f} (ë‹¬ì„± precision: {optimal_precision:.1%})")

        # ê²°ê³¼ ì €ì¥
        self.precision_threshold = optimal_threshold

        return {
            'precision_threshold': optimal_threshold,
            'achieved_precision': optimal_precision,
            'target_precision': target_precision,
            'precision_curve': precision_at_k.tolist()
        }

    def _tune_daily_top_k(self, df: pl.DataFrame, k: int = 5,
                         target_col: str = "big_move_event") -> Dict[str, Any]:
        """
        í•˜ë£¨ë§ˆë‹¤ í™•ë¥  ìƒìœ„ Kê°œë§Œ ì´ë²¤íŠ¸ë¡œ ê°„ì£¼í•˜ëŠ” ë°©ì‹ì˜ íŠœë‹

        Args:
            df: ì…ë ¥ ë°ì´í„°í”„ë ˆì„
            k: ìƒìœ„ Kê°œ
            target_col: íƒ€ê²Ÿ ì»¬ëŸ¼ëª…

        Returns:
            íŠœë‹ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print(f"[ì¼ë³„ ìƒìœ„ {k}ê°œ ì´ë²¤íŠ¸ ì„ íƒ]")

        # ë°ì´í„° ì¤€ë¹„
        pred_df = self._calculate_event_labels(df)
        available_features = [f for f in self.features if f in pred_df.columns]

        if not available_features:
            print("  [ê²½ê³ ] ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {'daily_top_k': k, 'results': {}}

        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        pred_df = pred_df.filter(~pl.any_horizontal(pl.col(available_features).is_null()))

        if len(pred_df) == 0:
            print("  [ê²½ê³ ] ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {'daily_top_k': k, 'results': {}}

        # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì˜ˆì¸¡
        dates = pred_df.select("date").unique().sort("date")

        total_events = 0
        total_predictions = 0
        daily_precisions = []

        for date_row in dates.rows():
            date = date_row[0]
            daily_df = pred_df.filter(pl.col("date") == date)

            if len(daily_df) == 0:
                continue

            # í•´ë‹¹ ë‚ ì§œ ë°ì´í„°ë¡œ ì˜ˆì¸¡
            daily_dataset = TimeSeriesDataset(
                df=daily_df,
                features=available_features,
                target_col=target_col,
                sequence_length=self.sequence_length,
                stride=1
            )

            daily_loader = DataLoader(daily_dataset, batch_size=64, shuffle=False, drop_last=False)

            daily_probs = []
            daily_targets = []

            with torch.no_grad():
                for batch_x, batch_y in daily_loader:
                    batch_x = batch_x.to(self.device)
                    outputs = self.model(batch_x)
                    probs = torch.softmax(outputs, dim=1)[:, 1]

                    daily_probs.extend(probs.cpu().numpy())
                    daily_targets.extend(batch_y.cpu().numpy())

            if len(daily_probs) == 0:
                continue

            # ìƒìœ„ kê°œ ì„ íƒ
            top_k_indices = np.argsort(daily_probs)[-k:]
            y_pred_daily = np.zeros(len(daily_probs))
            y_pred_daily[top_k_indices] = 1

            y_true_daily = np.array(daily_targets)

            # í•´ë‹¹ ë‚ ì§œì˜ precision ê³„ì‚°
            if y_pred_daily.sum() > 0:
                daily_precision = (y_true_daily & y_pred_daily).sum() / y_pred_daily.sum()
                daily_precisions.append(daily_precision)
                total_events += y_true_daily.sum()
                total_predictions += y_pred_daily.sum()

        # ì „ì²´ í‰ê·  precision
        if daily_precisions:
            avg_precision = np.mean(daily_precisions)
            print(f"  ì¼ë³„ í‰ê·  precision: {avg_precision:.1%}")
            print(f"  ì´ ì´ë²¤íŠ¸ ìˆ˜: {total_events}, ì´ ì˜ˆì¸¡ ìˆ˜: {total_predictions}")
        else:
            avg_precision = 0.0
        # ê²°ê³¼ ì €ì¥
        self.daily_top_k = k

        return {
            'daily_top_k': k,
            'avg_precision': avg_precision,
            'total_events': total_events,
            'total_predictions': total_predictions,
            'daily_precisions': daily_precisions
        }

    def _calibrate_probabilities(self, df: pl.DataFrame, method: str = "isotonic",
                               target_col: str = "big_move_event") -> Dict[str, Any]:
        """
        í™•ë¥  êµì • (Calibration) ìˆ˜í–‰

        Args:
            df: ì…ë ¥ ë°ì´í„°í”„ë ˆì„
            method: êµì • ë°©ë²• ("isotonic" ë˜ëŠ” "platt")
            target_col: íƒ€ê²Ÿ ì»¬ëŸ¼ëª…

        Returns:
            êµì • ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print(f"[í™•ë¥  êµì •] ë°©ë²•: {method}")

        # ë°ì´í„° ì¤€ë¹„
        pred_df = self._calculate_event_labels(df)
        available_features = [f for f in self.features if f in pred_df.columns]

        if not available_features:
            print("  [ê²½ê³ ] ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {'calibrator': None, 'results': {}}

        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        pred_df = pred_df.filter(~pl.any_horizontal(pl.col(available_features).is_null()))

        if len(pred_df) == 0:
            print("  [ê²½ê³ ] ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {'calibrator': None, 'results': {}}

        # ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ ìƒì„±
        pred_dataset = TimeSeriesDataset(
            df=pred_df,
            features=available_features,
            target_col=target_col,
            sequence_length=self.sequence_length,
            stride=1
        )

        pred_loader = DataLoader(pred_dataset, batch_size=64, shuffle=False, drop_last=False)

        # ëª¨ë¸ ì˜ˆì¸¡
        self.model.eval()
        all_probs = []
        all_targets = []

        with torch.no_grad():
            for batch_x, batch_y in pred_loader:
                batch_x = batch_x.to(self.device)
                outputs = self.model(batch_x)
                probs = torch.softmax(outputs, dim=1)[:, 1]

                all_probs.extend(probs.cpu().numpy())
                all_targets.extend(batch_y.cpu().numpy())

        y_true = np.array(all_targets)
        y_proba = np.array(all_probs)

        if len(y_true) == 0 or len(np.unique(y_true)) < 2:
            print("  [ê²½ê³ ] êµì •ì— ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {'calibrator': None, 'results': {}}

        # êµì • ìˆ˜í–‰
        if method == "isotonic":
            # Isotonic Regression
            calibrator = IsotonicRegression(out_of_bounds="clip")
            calibrator.fit(y_proba, y_true)

        elif method == "platt":
            # Platt scaling (Logistic Regression)
            calibrator = LogisticRegression()
            # í™•ë¥ ì„ logit ë³€í™˜
            eps = 1e-15
            y_proba_clipped = np.clip(y_proba, eps, 1 - eps)
            logits = np.log(y_proba_clipped / (1 - y_proba_clipped))
            calibrator.fit(logits.reshape(-1, 1), y_true)

        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” êµì • ë°©ë²•: {method}")

        # êµì •ëœ í™•ë¥  ê³„ì‚°
        if method == "isotonic":
            y_proba_calibrated = calibrator.predict(y_proba)
        else:  # platt
            eps = 1e-15
            y_proba_clipped = np.clip(y_proba, eps, 1 - eps)
            logits = np.log(y_proba_clipped / (1 - y_proba_clipped))
            platt_probs = calibrator.predict_proba(logits.reshape(-1, 1))[:, 1]
            y_proba_calibrated = platt_probs

        # êµì • ì„±ëŠ¥ í‰ê°€
        from sklearn.metrics import brier_score_loss

        # êµì • ì „/í›„ Brier score ë¹„êµ
        brier_before = brier_score_loss(y_true, y_proba)
        brier_after = brier_score_loss(y_true, y_proba_calibrated)

        print(f"  Brier Score - êµì • ì „: {brier_before:.4f}, êµì • í›„: {brier_after:.4f}")
        print(f"  ê°œì„ ëŸ‰: {brier_before - brier_after:.4f}")

        # êµì • ê²°ê³¼ ì €ì¥
        self.probability_calibrator = {
            'method': method,
            'calibrator': calibrator
        }

        return {
            'method': method,
            'calibrator': calibrator,
            'brier_before': brier_before,
            'brier_after': brier_after,
            'improvement': brier_before - brier_after,
            'original_probs': y_proba.tolist(),
            'calibrated_probs': y_proba_calibrated.tolist(),
            'true_labels': y_true.tolist()
        }

    def _apply_probability_calibration(self, probabilities: np.ndarray) -> np.ndarray:
        """
        í•™ìŠµëœ í™•ë¥  êµì • ì ìš©

        Args:
            probabilities: ì›ë³¸ í™•ë¥  ë°°ì—´

        Returns:
            êµì •ëœ í™•ë¥  ë°°ì—´
        """
        if self.probability_calibrator is None:
            print("  [ê²½ê³ ] í™•ë¥  êµì •ê¸°ê°€ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì›ë³¸ í™•ë¥ ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return probabilities

        method = self.probability_calibrator['method']
        calibrator = self.probability_calibrator['calibrator']

        if method == "isotonic":
            calibrated_probs = calibrator.predict(probabilities)

        elif method == "platt":
            eps = 1e-15
            probs_clipped = np.clip(probabilities, eps, 1 - eps)
            logits = np.log(probs_clipped / (1 - probs_clipped))
            platt_probs = calibrator.predict_proba(logits.reshape(-1, 1))[:, 1]
            calibrated_probs = platt_probs

        else:
            calibrated_probs = probabilities

        return calibrated_probs
    
    def load_data(self, market: str = "KR", years: List[int] = [2018, 2019, 2020], 
                  max_tickers: int = 100, normalize_features: bool = False) -> pl.DataFrame:
        """
        í•™ìŠµìš© ë°ì´í„° ë¡œë“œ
        
        Args:
            market: ì‹œì¥ ì½”ë“œ
            years: í•™ìŠµ ì—°ë„
            max_tickers: ìµœëŒ€ ì¢…ëª© ìˆ˜
            normalize_features: í”¼ì²˜ ì •ê·œí™” ì—¬ë¶€
            
        Returns:
            í•™ìŠµìš© ë°ì´í„°í”„ë ˆì„
        """
        print(f"[ë°ì´í„° ë¡œë“œ] {market} ì‹œì¥, {years} ì—°ë„, ìµœëŒ€ {max_tickers}ê°œ ì¢…ëª©")
        
        # ë°ì´í„°ì…‹ ë¹Œë“œ
        df = build_dataset(
            years=years,
            market=market,
            max_tickers=max_tickers,
            feature_set="v2",
            label_horizon=1,
            label_task="regression",  # íšŒê·€ë¡œ ë¡œë“œ í›„ ì§ì ‘ ë¼ë²¨ ê³„ì‚°
            verbose=False,
            normalize_features=normalize_features
        )
        
        print(f"  ë¡œë“œëœ ë°ì´í„°: {len(df):,} í–‰ Ã— {len(df.columns)} ì—´")
        
        # ì´ë²¤íŠ¸ ë¼ë²¨ ê³„ì‚° (ì´ˆê¸° thresholdë¡œ)
        df_with_events = self._calculate_event_labels(df)
        
        # k(ATR ë°°ìˆ˜) íŠœë‹ ìˆ˜í–‰ (íŠœë‹ëœ kë¡œ ë¼ë²¨ ì¬ê³„ì‚°)
        if len(years) > 1:  # ìµœì†Œ 2ê°œ ì´ìƒì˜ ì—°ë„ê°€ ìˆì–´ì•¼ CV ê°€ëŠ¥
            tuning_result = self._tune_threshold_with_cv(df)
            optimal_k = tuning_result['optimal_k']
            print(f"  íŠœë‹ëœ k: {optimal_k:.2f} (ê¸°ì¡´: {self.threshold:.2f})")

            # ìµœì  kë¡œ threshold ì—…ë°ì´íŠ¸
            self.threshold = optimal_k

            # íŠœë‹ëœ kë¡œ ë¼ë²¨ ì¬ê³„ì‚° (ì¼ê´€ì„± ìœ ì§€)
            print(f"  íŠœë‹ëœ k({optimal_k:.2f})ë¡œ ë¼ë²¨ ì¬ê³„ì‚° ì¤‘...")
            df_with_events = self._calculate_event_labels(df)
        

        
        # ì´ë²¤íŠ¸ í†µê³„ ì¶œë ¥
        event_stats = df_with_events.select([
            pl.col("big_move_event").sum().alias("events"),
            pl.len().alias("total")
        ]).row(0)

        print(f"  Intraday range ì´ë²¤íŠ¸ í†µê³„ (ì„ê³„ê°’: {self.threshold:.2f}):")
        print(f"    ì´ë²¤íŠ¸ ìˆ˜: {event_stats[0]:,}ê°œ ({event_stats[0]/event_stats[1]:.1%})")
        print(f"    ì´ë²¤íŠ¸ ë¼ë²¨: max(high/open-1, open/low-1) >= {self.threshold:.2f} Ã— ATR%")
        
        return df_with_events
    
    def train(self, df: pl.DataFrame, target_col: str = "big_move_event",
              test_size: float = 0.2, batch_size: int = 64, epochs: int = 50,
              learning_rate: float = 1e-3, patience: int = 10,
              use_class_balanced_sampler: bool = True) -> Dict[str, Any]:
        """
        TCN ëª¨ë¸ í•™ìŠµ

        Args:
            df: í•™ìŠµ ë°ì´í„°
            target_col: íƒ€ê²Ÿ ì»¬ëŸ¼ëª…
            test_size: í…ŒìŠ¤íŠ¸ ì…‹ ë¹„ìœ¨
            batch_size: ë°°ì¹˜ í¬ê¸°
            epochs: ìµœëŒ€ ì—í¬í¬ ìˆ˜
            learning_rate: í•™ìŠµë¥ 
            patience: early stopping patience
            use_class_balanced_sampler: í´ë˜ìŠ¤ ê· í˜• ìƒ˜í”ŒëŸ¬ ì‚¬ìš© ì—¬ë¶€

        Returns:
            í•™ìŠµ ê²°ê³¼
        """
        print(f"[TCN ëª¨ë¸ í•™ìŠµ] íƒ€ê²Ÿ: {target_col}")

        # ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ í™•ì¸
        available_features = [f for f in self.features if f in df.columns]
        missing_features = [f for f in self.features if f not in df.columns]

        if missing_features:
            print(f"  ëˆ„ë½ëœ í”¼ì²˜: {missing_features}")

        if not available_features:
            raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤!")

        print(f"  ì‚¬ìš© í”¼ì²˜: {len(available_features)}ê°œ")

        # ê²°ì¸¡ì¹˜ê°€ ì—†ëŠ” ë°ì´í„°ë§Œ ì‚¬ìš©
        train_df = df.filter(
            (~pl.col(target_col).is_null()) &
            (~pl.any_horizontal(pl.col(available_features).is_null()))
        )

        print(f"  í•™ìŠµ ë°ì´í„°: {len(train_df):,} í–‰")

        if len(train_df) == 0:
            raise ValueError("í•™ìŠµ ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")

        # ì‹œê°„ ê¸°ë°˜ train/validation split (ì‹œì  leakage ë°©ì§€)
        # ë¨¼ì € ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬
        train_df_sorted = train_df.sort("date")

        # ì „ì²´ ë°ì´í„°ë¥¼ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ë¶„í•  (ì‹œì  leakage ë°©ì§€)
        dates = train_df_sorted.select("date").to_pandas()['date'].values
        unique_dates = sorted(np.unique(dates))
        split_idx = int(len(unique_dates) * (1 - test_size))

        train_dates = unique_dates[:split_idx]
        valid_dates = unique_dates[split_idx:]

        # ë‚ ì§œ ê¸°ë°˜ ë¶„í•  (ë‹¤ì–‘í•œ í¬ë§· ì§€ì›)
        from datetime import date, datetime
        import pandas as pd

        def parse_date(d):
            if isinstance(d, date):
                return d
            try:
                # ISO í¬ë§· ë¬¸ìì—´ ì²˜ë¦¬
                date_str = str(d).split('T')[0] if 'T' in str(d) else str(d).split()[0]
                return date.fromisoformat(date_str)
            except (ValueError, AttributeError):
                # pandasë¥¼ ì‚¬ìš©í•œ ìœ ì—°í•œ íŒŒì‹±
                try:
                    return pd.to_datetime(d).date()
                except:
                    raise ValueError(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {d}")

        train_dates_dt = [parse_date(d) for d in train_dates]
        valid_dates_dt = [parse_date(d) for d in valid_dates]

        train_raw_df = train_df_sorted.filter(pl.col("date").is_in(train_dates_dt))
        valid_raw_df = train_df_sorted.filter(pl.col("date").is_in(valid_dates_dt))

        print(f"  í•™ìŠµ ë‚ ì§œ ë²”ìœ„: {train_dates[0]} ~ {train_dates[-1]}")
        print(f"  ê²€ì¦ ë‚ ì§œ ë²”ìœ„: {valid_dates[0]} ~ {valid_dates[-1]}")

        # ì´ë²¤íŠ¸ ë¼ë²¨ ì¬ê³„ì‚° (ë™ì¼í•œ threshold ì‚¬ìš©)
        train_processed_df = self._calculate_event_labels(train_raw_df)
        valid_processed_df = self._calculate_event_labels(valid_raw_df)

        # ì¬ê³„ì‚°ëœ ë¼ë²¨ë¡œ ë°ì´í„° ì¤€ë¹„
        X_train = train_processed_df.select(available_features).to_pandas()
        y_train = train_processed_df.select(target_col).to_pandas().iloc[:, 0].astype(int)
        X_valid = valid_processed_df.select(available_features).to_pandas()
        y_valid = valid_processed_df.select(target_col).to_pandas().iloc[:, 0].astype(int)

        # í‹°ì»¤ ì •ë³´ ì¶”ì¶œ
        train_tickers = train_processed_df.select("ticker").unique().to_pandas()['ticker'].tolist()
        valid_tickers = valid_processed_df.select("ticker").unique().to_pandas()['ticker'].tolist()

        print(f"  í•™ìŠµ ì´ë²¤íŠ¸ ë¹„ìœ¨: {y_train.mean():.1%} (ì´ {len(y_train)}ê°œ)")
        print(f"  ê²€ì¦ ì´ë²¤íŠ¸ ë¹„ìœ¨: {y_valid.mean():.1%} (ì´ {len(y_valid)}ê°œ)")

        # í´ë˜ìŠ¤ ë¶„í¬ ì¶œë ¥ (ì¬ê³„ì‚° í›„)
        train_class_counts = y_train.value_counts()
        valid_class_counts = y_valid.value_counts()

        # ì „ì²´ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¶„í¬ ê³„ì‚°
        all_targets = pd.concat([y_train, y_valid])
        all_class_counts = all_targets.value_counts()

        # None ê°’ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        train_neg = int(train_class_counts.get(0, 0) or 0)
        train_pos = int(train_class_counts.get(1, 0) or 0)
        valid_neg = int(valid_class_counts.get(0, 0) or 0)
        valid_pos = int(valid_class_counts.get(1, 0) or 0)

        # ì „ì²´ ë°ì´í„° í´ë˜ìŠ¤ ë¶„í¬ (ê²°ê³¼ ì €ì¥ìš©)
        class_counts_all = {
            'total_negative': int(all_class_counts.get(0, 0) or 0),
            'total_positive': int(all_class_counts.get(1, 0) or 0),
            'train_negative': train_neg,
            'train_positive': train_pos,
            'valid_negative': valid_neg,
            'valid_positive': valid_pos
        }

        print("  [ì¬ê³„ì‚°ëœ ë¼ë²¨ ë¶„í¬]")
        print(f"    Train - Negative (0): {train_neg:,}ê°œ, Positive (1): {train_pos:,}ê°œ ({train_pos / max(train_neg + train_pos, 1):.1%})")
        print(f"    Valid - Negative (0): {valid_neg:,}ê°œ, Positive (1): {valid_pos:,}ê°œ ({valid_pos / max(valid_neg + valid_pos, 1):.1%})")

        print(f"  í›ˆë ¨ í‹°ì»¤: {len(train_tickers)}ê°œ")
        print(f"  ê²€ì¦ í‹°ì»¤: {len(valid_tickers)}ê°œ")
        print(f"  í›ˆë ¨ ë°ì´í„°: {len(X_train):,} í–‰")
        print(f"  ê²€ì¦ ë°ì´í„°: {len(X_valid):,} í–‰")

        # í”¼ì²˜ ìŠ¤ì¼€ì¼ë§
        print("  ğŸ”„ í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ ì ìš© ì¤‘...")
        self.scaler.fit(X_train)
        X_train_scaled = self.scaler.transform(X_train)
        X_valid_scaled = self.scaler.transform(X_valid)

        # ìŠ¤ì¼€ì¼ë§ëœ í”¼ì²˜ë¡œ ë°ì´í„°í”„ë ˆì„ ì—…ë°ì´íŠ¸
        print("  ğŸ“Š ìŠ¤ì¼€ì¼ë§ëœ í”¼ì²˜ë¡œ ë°ì´í„°í”„ë ˆì„ ì—…ë°ì´íŠ¸")
        train_scaled_df = train_processed_df.with_columns([
            pl.Series(name=f, values=X_train_scaled[:, i], dtype=pl.Float64)
            for i, f in enumerate(available_features)
        ])

        valid_scaled_df = valid_processed_df.with_columns([
            pl.Series(name=f, values=X_valid_scaled[:, i], dtype=pl.Float64)
            for i, f in enumerate(available_features)
        ])

        # PyTorch ë°ì´í„°ì…‹ ìƒì„± (ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° ì‚¬ìš©)
        train_dataset = TimeSeriesDataset(
            df=train_scaled_df,
            features=available_features,
            target_col=target_col,
            sequence_length=self.sequence_length
        )

        valid_dataset = TimeSeriesDataset(
            df=valid_scaled_df,
            features=available_features,
            target_col=target_col,
            sequence_length=self.sequence_length
        )

        # DataLoader ìƒì„±
        if use_class_balanced_sampler:
            # í´ë˜ìŠ¤ ê· í˜• ìƒ˜í”ŒëŸ¬ ì‚¬ìš©
            train_targets = torch.tensor([train_dataset[i][1] for i in range(len(train_dataset))])
            train_sampler = self._create_class_balanced_sampler(train_targets)
            train_loader = DataLoader(train_dataset, batch_size=batch_size,
                                    sampler=train_sampler, drop_last=True)
        else:
            train_loader = DataLoader(train_dataset, batch_size=batch_size,
                                    shuffle=True, drop_last=True)

        valid_loader = DataLoader(valid_dataset, batch_size=batch_size,
                                shuffle=False, drop_last=False)

        # TCN ëª¨ë¸ ìƒì„±
        num_features = len(available_features)
        self.model = EventDetectorTCN(
            num_features=num_features,
            sequence_length=self.sequence_length,
            num_channels=self.num_channels,
            dropout=self.dropout
        ).to(self.device)

        # Focal Lossì™€ ìµœì í™” ì„¤ì •
        criterion = FocalLoss(alpha=self.focal_loss_alpha, gamma=self.focal_loss_gamma)
        optimizer = Adam(self.model.parameters(), lr=learning_rate)
        scheduler = CosineAnnealingLR(optimizer, T_max=epochs)

        # í•™ìŠµ ë£¨í”„
        best_f1 = 0.0
        best_model_state = None
        patience_counter = 0

        print("  TCN í•™ìŠµ ì‹œì‘...")
        for epoch in range(epochs):
            # í•™ìŠµ
            self.model.train()
            train_loss = 0.0

            for batch_x, batch_y in train_loader:
                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)

                optimizer.zero_grad()
                outputs = self.model(batch_x)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()

                train_loss += loss.item()

            train_loss /= len(train_loader)

            # ê²€ì¦
            self.model.eval()
            valid_loss = 0.0
            all_preds = []
            all_targets = []
            all_probs = []

            with torch.no_grad():
                for batch_x, batch_y in valid_loader:
                    batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)

                    outputs = self.model(batch_x)
                    loss = criterion(outputs, batch_y)
                    valid_loss += loss.item()

                    probs = torch.softmax(outputs, dim=1)[:, 1]
                    preds = torch.argmax(outputs, dim=1)

                    all_preds.extend(preds.cpu().numpy())
                    all_targets.extend(batch_y.cpu().numpy())
                    all_probs.extend(probs.cpu().numpy())

            # ë¦¬ìŠ¤íŠ¸ë¥¼ numpy arrayë¡œ ë³€í™˜
            all_targets = np.array(all_targets)
            all_preds = np.array(all_preds)
            all_probs = np.array(all_probs)

            valid_loss /= len(valid_loader)

            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            accuracy = (all_preds == all_targets).mean()
            f1 = f1_score(all_targets, all_preds, zero_division=0)

            # ì¶”ê°€ ì§€í‘œë“¤ ê³„ì‚°
            if len(np.unique(all_targets)) > 1:
                roc_auc = roc_auc_score(all_targets, all_probs)
                precision, recall, _ = precision_recall_curve(all_targets, all_probs)
                pr_auc = auc(recall, precision)

                # Precision, Recall ê³„ì‚°
                p_score = precision_score(all_targets, all_preds, zero_division=0)
                r_score = recall_score(all_targets, all_preds, zero_division=0)

                # Balanced Accuracy
                balanced_acc = balanced_accuracy_score(all_targets, all_preds)

                # Precision@k ê³„ì‚° (í•™ìŠµ ì¤‘ ëª¨ë‹ˆí„°ë§ìš©)
                precision_at_k_results = {}
                if len(all_targets) > 0 and all_targets.sum() > 0:
                    k_values = [1, 3, 5, 10]
                    for k in k_values:
                        if k <= len(all_probs):
                            # ìƒìœ„ kê°œ ì˜ˆì¸¡ ì„ íƒ
                            top_k_indices = np.argsort(all_probs)[-k:]
                            y_pred_top_k = np.zeros(len(all_probs))
                            y_pred_top_k[top_k_indices] = 1

                            # precision@k ê³„ì‚°
                            if y_pred_top_k.sum() > 0:
                                # íƒ€ì… ë³€í™˜ í›„ bitwise AND ì—°ì‚°
                                y_true_int = all_targets.astype(int)
                                y_pred_int = y_pred_top_k.astype(int)
                                precision_at_k = (y_true_int & y_pred_int).sum() / y_pred_top_k.sum()
                            else:
                                precision_at_k = 0.0

                            precision_at_k_results[f'precision@{k}'] = precision_at_k
            else:
                roc_auc = 0.0
                pr_auc = 0.0
                p_score = 0.0
                r_score = 0.0
                balanced_acc = 0.0
                precision_at_k_results = {}

            if (epoch + 1) % 5 == 0:
                print(f"    Epoch {epoch+1:3d}/{epochs} | Train Loss: {train_loss:.4f} | Valid Loss: {valid_loss:.4f}")
                print(f"      F1: {f1:.4f} | Precision: {p_score:.4f} | Recall: {r_score:.4f}")
                print(f"      ROC-AUC: {roc_auc:.4f} | PR-AUC: {pr_auc:.4f} | Balanced Acc: {balanced_acc:.4f}")

                # Precision@k ì¶œë ¥ (5ì—í¬í¬ë§ˆë‹¤)
                if precision_at_k_results:
                    print(f"      ğŸ“Š Precision@5: {precision_at_k_results.get('precision@5', 0.0):.4f} | "
                          f"@10: {precision_at_k_results.get('precision@10', 0.0):.4f}")

            # Early stopping
            if f1 > best_f1:
                best_f1 = f1
                best_model_state = self.model.state_dict().copy()
                patience_counter = 0
            else:
                patience_counter += 1

            if patience_counter >= patience:
                print(f"    Early stopping at epoch {epoch+1}")
                break

            scheduler.step()

        # ìµœì  ëª¨ë¸ ë³µì›
        if best_model_state is not None:
            self.model.load_state_dict(best_model_state)

        print("[í•™ìŠµ ê²°ê³¼]")
        print(f"  ì •í™•ë„: {accuracy:.3f}")
        print(f"  F1 ìŠ¤ì½”ì–´: {best_f1:.3f}")
        print(f"  Precision: {p_score:.3f}")
        print(f"  Recall: {r_score:.3f}")
        print(f"  ROC-AUC: {roc_auc:.3f}")
        print(f"  PR-AUC: {pr_auc:.3f}")
        print(f"  Balanced Accuracy: {balanced_acc:.3f}")

        # Calibration ì ìš© (ì„ íƒì‚¬í•­)
        if hasattr(self, 'apply_calibration_after_training') and self.apply_calibration_after_training:
            print("\n[í•™ìŠµ í›„ Calibration ì ìš©]")
            try:
                # ê²€ì¦ ë°ì´í„°ë¡œ í™•ë¥  êµì • ìˆ˜í–‰
                calib_result = self._calibrate_probabilities(valid_raw_df, method="isotonic", target_col=target_col)

                # Precision ê¸°ë°˜ threshold íŠœë‹
                precision_result = self._tune_threshold_with_precision(valid_raw_df, target_precision=self.target_precision, target_col=target_col)

                # Daily top-k íŠœë‹ (ì„ íƒì‚¬í•­)
                if self.daily_top_k is None:  # daily_top_kê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ
                    daily_k_result = self._tune_daily_top_k(valid_raw_df, k=5, target_col=target_col)
                    print(f"  ğŸ“Š Daily top-k: {daily_k_result.get('avg_precision', 'N/A'):.3f}")

                print(f"  ğŸ“Š Calibration ì™„ë£Œ - Brier ê°œì„ : {calib_result.get('improvement', 0):.4f}")
                print(f"  ğŸ“Š Precision threshold: {precision_result.get('precision_threshold', 'N/A')}")

            except Exception as e:
                print(f"  [ê²½ê³ ] Calibration ì‹¤íŒ¨: {e}")

        # ê²°ê³¼ ì €ì¥
        results = {
            'model': self.model,
            'features': available_features,
            'target': target_col,
            'accuracy': accuracy,
            'f1_score': best_f1,
            'roc_auc': roc_auc,
            'pr_auc': pr_auc,
            'class_distribution': class_counts_all,
            'threshold': self.threshold,
            'epochs_trained': epoch + 1,
            'best_f1': best_f1,
            'calibration_applied': self.probability_calibrator is not None,
            'precision_threshold': self.precision_threshold,
            'daily_top_k': self.daily_top_k
        }

        self.training_history = results

        return results
    
    def predict(self, df: pl.DataFrame, batch_size: int = 64) -> Tuple[np.ndarray, np.ndarray]:
        """
        TCN ê¸°ë°˜ ì˜ˆì¸¡ ìˆ˜í–‰ (í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ìŠ¤ì¼€ì¼ëŸ¬ ì ìš©)

        Args:
            df: ì˜ˆì¸¡ ë°ì´í„°
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            (ì˜ˆì¸¡ í´ë˜ìŠ¤, ì˜ˆì¸¡ í™•ë¥ )
        """
        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")

        # ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ í™•ì¸
        available_features = [f for f in self.features if f in df.columns]

        if not available_features:
            print("  [ê²½ê³ ] ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return np.zeros(len(df)), np.zeros(len(df))

        # ê²°ì¸¡ì¹˜ê°€ ì—†ëŠ” ë°ì´í„°ë§Œ ì‚¬ìš©
        pred_df = df.filter(
            ~pl.any_horizontal(pl.col(available_features).is_null())
        )

        if len(pred_df) == 0:
            print("  [ê²½ê³ ] ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return np.zeros(len(df)), np.zeros(len(df))

        print(f"  ì˜ˆì¸¡ ë°ì´í„°: {len(pred_df):,} í–‰")

        # ì˜ˆì¸¡ ì‹œì—ëŠ” ì´ë²¤íŠ¸ ë¼ë²¨ ê³„ì‚° ìƒëµ (ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©)
        # pred_df = self._calculate_event_labels(pred_df)

        # ğŸ“Š ìŠ¤ì¼€ì¼ëŸ¬ ì ìš© (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ìŠ¤ì¼€ì¼ë§)
        if self.scaler is not None:
            print("  ğŸ”„ í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ìŠ¤ì¼€ì¼ëŸ¬ ì ìš© ì¤‘...")
            # í”¼ì²˜ ë°ì´í„°ë¥¼ pandasë¡œ ë³€í™˜í•˜ì—¬ ìŠ¤ì¼€ì¼ë§
            X_pred = pred_df.select(available_features).to_pandas()
            X_pred_scaled = self.scaler.transform(X_pred)

            # ìŠ¤ì¼€ì¼ë§ëœ í”¼ì²˜ë¡œ ë°ì´í„°í”„ë ˆì„ ì—…ë°ì´íŠ¸ (ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ)
            print("  ğŸ“Š ìŠ¤ì¼€ì¼ë§ëœ í”¼ì²˜ë¡œ ì˜ˆì¸¡ ë°ì´í„°í”„ë ˆì„ ì—…ë°ì´íŠ¸")
            pred_df = pred_df.with_columns([
                pl.Series(name=f, values=X_pred_scaled[:, i], dtype=pl.Float64)
                for i, f in enumerate(available_features)
            ])

        # ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ ìƒì„± (ì˜ˆì¸¡ ì‹œì—ëŠ” íƒ€ê²Ÿ ì»¬ëŸ¼ ì—†ì´)
        pred_dataset = TimeSeriesDataset(
            df=pred_df,
            features=available_features,
            target_col=None,  # ì˜ˆì¸¡ ì‹œì—ëŠ” íƒ€ê²Ÿ ì»¬ëŸ¼ ë¶ˆí•„ìš”
            sequence_length=self.sequence_length,
            stride=1
        )

        pred_loader = DataLoader(pred_dataset, batch_size=batch_size,
                               shuffle=False, drop_last=False)

        # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        self.model.eval()

        all_probs = []
        all_preds = []

        with torch.no_grad():
            for batch_x, _ in pred_loader:
                batch_x = batch_x.to(self.device)

                outputs = self.model(batch_x)
                probs = torch.softmax(outputs, dim=1)[:, 1]
                preds = torch.argmax(outputs, dim=1)

                all_probs.extend(probs.cpu().numpy())
                all_preds.extend(preds.cpu().numpy())

        # ì „ì²´ ë°ì´í„° ê¸¸ì´ì— ë§ê²Œ ê²°ê³¼ í™•ì¥ (ì•ˆì „í•œ ì¸ë±ìŠ¤ ë§¤í•‘)
        y_pred_proba = np.full(len(df), np.nan)  # NaNìœ¼ë¡œ ì´ˆê¸°í™”í•˜ì—¬ ì˜ˆì¸¡ë˜ì§€ ì•Šì€ í–‰ êµ¬ë¶„
        y_pred = np.full(len(df), -1)  # -1ë¡œ ì´ˆê¸°í™”í•˜ì—¬ ì˜ˆì¸¡ë˜ì§€ ì•Šì€ í–‰ êµ¬ë¶„

        # nullì´ ì—†ëŠ” í–‰ë“¤ì˜ ì¸ë±ìŠ¤ ê³„ì‚°
        mask = df.select(
            pl.any_horizontal([pl.col(c).is_null() for c in available_features]).alias("row_has_null")
        )["row_has_null"]
        valid_indices = np.where(~mask.to_numpy())[0]

        # ì˜ˆì¸¡ ê²°ê³¼ì™€ valid_indicesì˜ ê¸¸ì´ í™•ì¸ ë° ì¡°ì •
        num_predictions = min(len(all_probs), len(valid_indices))

        # ì•ˆì „í•˜ê²Œ ì¸ë±ìŠ¤ ë§¤í•‘
        for i in range(num_predictions):
            if i < len(valid_indices):
                data_idx = valid_indices[i]
                if data_idx < len(df):
                    y_pred_proba[data_idx] = all_probs[i]
                    y_pred[data_idx] = all_preds[i]
        
        return y_pred.astype(int), y_pred_proba
    
    def evaluate(self, df: pl.DataFrame, target_col: str = "big_move_event",
                 batch_size: int = 64) -> Dict[str, Any]:
        """
        TCN ëª¨ë¸ í‰ê°€ (ë™ì¼í•œ ì´ë²¤íŠ¸ ë¼ë²¨ ê³„ì‚° ì ìš©)

        Args:
            df: í‰ê°€ ë°ì´í„° (ì´ë²¤íŠ¸ ë¼ë²¨ì´ ê³„ì‚°ë˜ì§€ ì•Šì€ raw ë°ì´í„°)
            target_col: íƒ€ê²Ÿ ì»¬ëŸ¼ëª…
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            í‰ê°€ ê²°ê³¼
        """
        print(f"[TCN ëª¨ë¸ í‰ê°€] íƒ€ê²Ÿ: {target_col}")

        # ì´ë²¤íŠ¸ ë¼ë²¨ ê³„ì‚° (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ë¡œì§ ì ìš©)
        eval_df = self._calculate_event_labels(df)

        # ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ í™•ì¸
        available_features = [f for f in self.features if f in eval_df.columns]

        if not available_features:
            print("  [ê²½ê³ ] ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return {'accuracy': 0.0, 'roc_auc': 0.0}

        # ê²°ì¸¡ì¹˜ê°€ ì—†ëŠ” ë°ì´í„°ë§Œ ì‚¬ìš©
        eval_df = eval_df.filter(
            (~pl.col(target_col).is_null()) &
            (~pl.any_horizontal(pl.col(available_features).is_null()))
        )

        if len(eval_df) == 0:
            print("  [ê²½ê³ ] í‰ê°€ ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return {'accuracy': 0.0, 'roc_auc': 0.0}

        print(f"  í‰ê°€ ë°ì´í„°: {len(eval_df):,} í–‰")

        # ğŸ“Š ìŠ¤ì¼€ì¼ëŸ¬ ì ìš© (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ìŠ¤ì¼€ì¼ë§)
        if self.scaler is not None:
            print("  ğŸ”„ í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ í‰ê°€ ë°ì´í„° ìŠ¤ì¼€ì¼ë§")
            # í”¼ì²˜ ë°ì´í„°ë¥¼ pandasë¡œ ë³€í™˜í•˜ì—¬ ìŠ¤ì¼€ì¼ë§
            X_eval = eval_df.select(available_features).to_pandas()
            X_eval_scaled = self.scaler.transform(X_eval)

            # ìŠ¤ì¼€ì¼ë§ëœ í”¼ì²˜ë¡œ ë°ì´í„°í”„ë ˆì„ ì—…ë°ì´íŠ¸
            print("  ğŸ“Š ìŠ¤ì¼€ì¼ë§ëœ í”¼ì²˜ë¡œ í‰ê°€ ë°ì´í„°í”„ë ˆì„ ì—…ë°ì´íŠ¸")
            eval_df = eval_df.with_columns([
                pl.Series(name=f, values=X_eval_scaled[:, i], dtype=pl.Float64)
                for i, f in enumerate(available_features)
            ])

        # ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ ìƒì„±
        eval_dataset = TimeSeriesDataset(
            df=eval_df,
            features=available_features,
            target_col=target_col,
            sequence_length=self.sequence_length,
            stride=1
        )

        eval_loader = DataLoader(eval_dataset, batch_size=batch_size,
                               shuffle=False, drop_last=False)

        # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        self.model.eval()

        all_targets = []
        all_probs = []
        all_preds = []

        with torch.no_grad():
            for batch_x, batch_y in eval_loader:
                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)

                outputs = self.model(batch_x)
                probs = torch.softmax(outputs, dim=1)[:, 1]
                preds = torch.argmax(outputs, dim=1)

                all_targets.extend(batch_y.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())
                all_preds.extend(preds.cpu().numpy())

        # í‰ê°€ ì§€í‘œ ê³„ì‚°
        y_true = np.array(all_targets)
        y_pred = np.array(all_preds)
        y_pred_proba = np.array(all_probs)

        # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
        unique_classes = np.unique(y_true)
        if len(y_true) > 0:
            class_counts = np.bincount(y_true.astype(int), minlength=2)
            print(f"  í´ë˜ìŠ¤ ë¶„í¬: {class_counts}")
        else:
            print(f"  í´ë˜ìŠ¤ ë¶„í¬: [0 0] (ë¹ˆ ë°ì´í„°)")

        # ê¸°ë³¸ ì§€í‘œ
        accuracy = (y_pred == y_true).mean()
        f1 = f1_score(y_true, y_pred, zero_division=0)

        # ROC-AUCì™€ PR-AUC ê³„ì‚°
        roc_auc = 0.0
        pr_auc = 0.0

        if len(unique_classes) > 1:
            roc_auc = roc_auc_score(y_true, y_pred_proba)
            precision_vals, recall_vals, _ = precision_recall_curve(y_true, y_pred_proba)
            pr_auc = auc(recall_vals, precision_vals)

            # ì¶”ê°€ ì§€í‘œ ê³„ì‚°
            precision = precision_score(y_true, y_pred, zero_division=0)
            recall = recall_score(y_true, y_pred, zero_division=0)
            balanced_acc = balanced_accuracy_score(y_true, y_pred)

            print(f"  ì •í™•ë„: {accuracy:.3f}")
            print(f"  F1 ìŠ¤ì½”ì–´: {f1:.3f}")
            print(f"  Precision: {precision:.3f}")
            print(f"  Recall: {recall:.3f}")
            print(f"  ROC-AUC: {roc_auc:.3f}")
            print(f"  PR-AUC: {pr_auc:.3f}")
            print(f"  Balanced Accuracy: {balanced_acc:.3f}")
        else:
            print(f"  ì •í™•ë„: {accuracy:.3f}")
            print(f"  F1 ìŠ¤ì½”ì–´: {f1:.3f}")
            print(f"  AUC: ê³„ì‚° ë¶ˆê°€ (ë‹¨ì¼ í´ë˜ìŠ¤)")

        # ë¶„ë¥˜ ë¦¬í¬íŠ¸
        try:
            print("ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
            print(classification_report(y_true, y_pred))
        except Exception as e:
            print(f"  ë¶„ë¥˜ ë¦¬í¬íŠ¸: ìƒì„± ë¶ˆê°€ ({e})")

        # Precision@k ê¸°ë°˜ í‰ê°€ ì¶”ê°€
        precision_at_k_results = {}

        # ë‹¤ì–‘í•œ k ê°’ì— ëŒ€í•œ precision@k ê³„ì‚°
        if len(y_true) > 0 and y_true.sum() > 0:
            k_values = [1, 3, 5, 10, min(20, len(y_true))]
            for k in k_values:
                if k <= len(y_pred_proba):
                    # ìƒìœ„ kê°œ ì˜ˆì¸¡ ì„ íƒ
                    top_k_indices = np.argsort(y_pred_proba)[-k:]
                    y_pred_top_k = np.zeros(len(y_pred_proba))
                    y_pred_top_k[top_k_indices] = 1

                    # precision@k ê³„ì‚°
                    if y_pred_top_k.sum() > 0:
                        # ë‘ ë°°ì—´ì„ ê°™ì€ íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ bitwise AND ì—°ì‚°
                        y_true_int = y_true.astype(int)
                        y_pred_int = y_pred_top_k.astype(int)
                        precision_at_k = (y_true_int & y_pred_int).sum() / y_pred_top_k.sum()
                    else:
                        precision_at_k = 0.0

                    precision_at_k_results[f'precision@{k}'] = precision_at_k

                    if k == 5:  # ê¸°ë³¸ k=5ì— ëŒ€í•´ ìì„¸íˆ ì¶œë ¥
                        # recall@k ê³„ì‚° (íƒ€ì… ë³€í™˜ í›„)
                        y_true_int = y_true.astype(int)
                        y_pred_int = y_pred_top_k.astype(int)
                        recall_at_k = (y_true_int & y_pred_int).sum() / y_true.sum() if y_true.sum() > 0 else 0.0
                        print(f"  Precision@5: {precision_at_k:.3f}")
                        print(f"  Recall@5: {recall_at_k:.3f}")

        return {
            'accuracy': accuracy,
            'f1_score': f1,
            'precision': precision,
            'recall': recall,
            'roc_auc': roc_auc,
            'pr_auc': pr_auc,
            'balanced_accuracy': balanced_acc,
            'precision_at_k': precision_at_k_results,
            'y_true': y_true,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba
        }
    
    def plot_feature_importance(self, top_n: int = 20, show: bool = True):
        """í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™”"""
        if self.feature_importance_ is None:
            print("í”¼ì²˜ ì¤‘ìš”ë„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.")
            return
        
        plt.figure(figsize=(10, 8))
        top_features = self.feature_importance_.head(top_n)
        
        sns.barplot(data=top_features, x='importance', y='feature')
        plt.title(f'Top {top_n} Feature Importance (Event Detector)')
        plt.xlabel('Importance')
        plt.tight_layout()
        
        if show:
            plt.show()
        else:
            plt.close()
    
    def plot_event_distribution(self, df: pl.DataFrame, show: bool = True):
        """Intraday range ê¸°ë°˜ ì´ë²¤íŠ¸ ë¶„í¬ ì‹œê°í™”"""
        print("[Intraday range ì´ë²¤íŠ¸ ë¶„í¬ ë¶„ì„]")

        # ì´ë²¤íŠ¸ ë¼ë²¨ ê³„ì‚° (í•­ìƒ ì¬ê³„ì‚°í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€)
        df = self._calculate_event_labels(df)

        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # 1. Intraday range ë¶„í¬
        intraday_ranges = df['next_day_intraday_range'].to_pandas()
        finite_ranges = intraday_ranges[np.isfinite(intraday_ranges)]
        axes[0, 0].hist(finite_ranges * 100, bins=50, alpha=0.7)
        axes[0, 0].set_xlabel('Intraday Range (%)')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].set_title('Intraday Range Distribution')
        axes[0, 0].grid(True, alpha=0.3)

        # 2. ATR% ë¶„í¬
        atr_percent = df['atr_percent'].to_pandas()
        finite_atr = atr_percent[np.isfinite(atr_percent)]
        axes[0, 1].hist(finite_atr * 100, bins=50, alpha=0.7)
        axes[0, 1].axvline(self.threshold * np.median(finite_atr) * 100, color='red', linestyle='--',
                          label=f'Threshold Ã— Median ATR%')
        axes[0, 1].set_xlabel('ATR (%)')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].set_title('ATR Distribution')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)

        # 3. ì´ë²¤íŠ¸ë³„ ì›”ë³„ ë¶„í¬
        df_pd = df.to_pandas()
        df_pd['year_month'] = pd.to_datetime(df_pd['date']).dt.to_period('M')
        monthly_events = df_pd.groupby('year_month')['big_move_event'].sum()
        axes[1, 0].plot(monthly_events.index.astype(str), monthly_events.values, marker='o')
        axes[1, 0].set_xlabel('Month')
        axes[1, 0].set_ylabel('Event Count')
        axes[1, 0].set_title('Monthly Event Count')
        axes[1, 0].tick_params(axis='x', rotation=45)
        axes[1, 0].grid(True, alpha=0.3)

        # 4. ì´ë²¤íŠ¸ í¬ê¸° ë¶„í¬ (Intraday Range / ATR%)
        event_sizes = (df['next_day_intraday_range'] / df['atr_percent']).to_pandas()
        finite_events = event_sizes[np.isfinite(event_sizes) & (event_sizes >= self.threshold)]
        if len(finite_events) > 0:
            axes[1, 1].hist(finite_events, bins=30, alpha=0.7)
            axes[1, 1].axvline(self.threshold, color='red', linestyle='--', label=f'Threshold ({self.threshold:.2f})')
            axes[1, 1].set_xlabel('Event Size (Intraday Range / ATR%)')
            axes[1, 1].set_ylabel('Frequency')
            axes[1, 1].set_title('Event Size Distribution')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)

        plt.suptitle(f'Intraday Range Event Analysis (Threshold: {self.threshold:.2f})', fontsize=14)
        plt.tight_layout()

        if show:
            plt.show()
        else:
            plt.close()
    
    def save_model(self, path: str):
        """TCN ëª¨ë¸ ì €ì¥"""
        if self.model is None:
            raise ValueError("ì €ì¥í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")

        model_path = Path(path)
        model_path.parent.mkdir(parents=True, exist_ok=True)

        # PyTorch ëª¨ë¸ ì €ì¥
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'scaler': self.scaler
        }, str(model_path.with_suffix('.pth')))

        # ë©”íƒ€ë°ì´í„° ì €ì¥ (JSONìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì €ì¥)
        import json
        metadata = {
            'threshold': self.threshold,
            'sequence_length': self.sequence_length,
            'dropout': self.dropout,
            'num_channels': self.num_channels,
            'features': self.features,
            'device': str(self.device),
            'focal_loss_alpha': self.focal_loss_alpha,
            'focal_loss_gamma': self.focal_loss_gamma,
            'training_history': self.training_history
        }

        # JSONìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ì €ì¥ (pickle ë¬¸ì œ íšŒí”¼)
        with open(model_path.with_suffix('.json'), 'w') as f:
            json.dump(metadata, f, indent=2, default=str)

        # ê¸°ì¡´ pickle ë°©ì‹ë„ ìœ ì§€ (í˜¸í™˜ì„±)
        try:
            with open(model_path.with_suffix('.pkl'), 'wb') as f:
                pickle.dump(metadata, f)
        except Exception as e:
            print(f"  [ê²½ê³ ] Pickle ì €ì¥ ì‹¤íŒ¨ (JSONìœ¼ë¡œ ì €ì¥ë¨): {e}")

        print(f"[TCN ëª¨ë¸ ì €ì¥] {model_path}")

    def load_model(self, path: str):
        """TCN ëª¨ë¸ ë¡œë“œ"""
        model_path = Path(path)
        import json

        # ë©”íƒ€ë°ì´í„° ë¡œë“œ (JSON ìš°ì„ , pickle fallback)
        metadata = None

        # 1. JSON íŒŒì¼ ì‹œë„ (ì•ˆì „í•œ ë°©ì‹)
        json_path = model_path.with_suffix('.json')
        if json_path.exists():
            try:
                with open(json_path, 'r') as f:
                    metadata = json.load(f)
                print(f"  âœ… JSON ë©”íƒ€ë°ì´í„° ë¡œë“œ ì„±ê³µ")
            except Exception as e:
                print(f"  [ê²½ê³ ] JSON ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

        # 2. Pickle íŒŒì¼ ì‹œë„ (ê¸°ì¡´ í˜¸í™˜ì„±)
        if metadata is None:
            pickle_path = model_path.with_suffix('.pkl')
            if pickle_path.exists():
                try:
                    # ëª¨ë¸ í´ë˜ìŠ¤ë“¤ì„ importí•˜ì—¬ pickleì´ ì°¾ì„ ìˆ˜ ìˆë„ë¡ í•¨
                    from models.M001_EventDetector import EventDetectorTCN, FocalLoss

                    with open(pickle_path, 'rb') as f:
                        metadata = pickle.load(f)
                    print(f"  âœ… Pickle ë©”íƒ€ë°ì´í„° ë¡œë“œ ì„±ê³µ")
                except Exception as e:
                    print(f"  [ê²½ê³ ] Pickle ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

        # 3. ë©”íƒ€ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ëª¨ë¸ íŒŒì¼ì—ì„œ ì¶”ì¶œ
        if metadata is None:
            print("  [ê²½ê³ ] ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ì–´ ëª¨ë¸ íŒŒì¼ì—ì„œ ì •ë³´ ì¶”ì¶œ ì‹œë„...")
            checkpoint = torch.load(str(model_path.with_suffix('.pth')), map_location=self.device)
            if 'scaler' in checkpoint:
                self.scaler = checkpoint['scaler']
                print(f"  âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì •ë³´ ì¶”ì¶œ ì„±ê³µ")
            else:
                print("  [ê²½ê³ ] ìŠ¤ì¼€ì¼ëŸ¬ ì •ë³´ê°€ ëª¨ë¸ íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤.")

            # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì„¤ì •
            metadata = {
                'threshold': getattr(self, 'threshold', 1.0),
                'sequence_length': getattr(self, 'sequence_length', 60),
                'dropout': getattr(self, 'dropout', 0.2),
                'num_channels': getattr(self, 'num_channels', [160, 192, 224, 256, 256]),
                'features': getattr(self, 'features', []),
                'device': str(self.device),
                'focal_loss_alpha': getattr(self, 'focal_loss_alpha', 1.0),
                'focal_loss_gamma': getattr(self, 'focal_loss_gamma', 2.0),
                'training_history': {}
            }

        # ë©”íƒ€ë°ì´í„°ë¡œ ì´ˆê¸°í™”
        self.threshold = metadata['threshold']
        self.sequence_length = metadata['sequence_length']
        self.dropout = metadata['dropout']
        self.num_channels = metadata['num_channels']
        self.features = metadata['features']
        self.device = torch.device(metadata.get('device', 'cpu'))
        self.focal_loss_alpha = metadata.get('focal_loss_alpha', 1.0)
        self.focal_loss_gamma = metadata.get('focal_loss_gamma', 2.0)
        self.training_history = metadata.get('training_history', {})

        # PyTorch ëª¨ë¸ ë¡œë“œ
        checkpoint = torch.load(str(model_path.with_suffix('.pth')), map_location=self.device)

        # TCN ëª¨ë¸ ìƒì„± ë° ë¡œë“œ
        num_features = len(self.features)
        self.model = EventDetectorTCN(
            num_features=num_features,
            sequence_length=self.sequence_length,
            num_channels=self.num_channels,
            dropout=self.dropout
        ).to(self.device)

        self.model.load_state_dict(checkpoint['model_state_dict'])

        # Scaler ë¡œë“œ
        self.scaler = checkpoint['scaler']

        print(f"[TCN ëª¨ë¸ ë¡œë“œ] {model_path}")
        print(f"  ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"  ì‹œí€€ìŠ¤ ê¸¸ì´: {self.sequence_length}")
        print(f"  í”¼ì²˜ ìˆ˜: {len(self.features)}")
        print(f"  TCN ì±„ë„: {self.num_channels}")


def create_event_detector_model(market: str = "KR", years: List[int] = [2018, 2019, 2020],
                               threshold: float = 1.0, target: str = "big_move_event",
                               max_tickers: int = 100, save_model: bool = True,
                               sequence_length: int = 60, batch_size: int = 64,
                               epochs: int = 50, learning_rate: float = 1e-3,
                               use_class_balanced_sampler: bool = True,
                               apply_calibration: bool = True,
                               daily_top_k: int = 5, target_precision: float = 0.4) -> EventDetectorManager:
    """
    Intraday range ê¸°ë°˜ TCN Event Detector ëª¨ë¸ ìƒì„± ë° í•™ìŠµ

    ì´ë²¤íŠ¸ ë¼ë²¨: max(high/open-1, open/low-1) >= k Ã— ATR%
    ATRì„ ìˆ˜ìµë¥  ë‹¨ìœ„ë¡œ ë³€í™˜í•˜ì—¬ ì¢…ëª©ë³„Â·ë ˆì§ë³„ ë³€ë™ì„± ë³´ì •
    ì—°ë„ë³„ CVë¥¼ í†µí•´ ìµœì  k ìë™ íŠœë‹
    í™•ë¥  êµì • ë° Precision@k ê¸°ë°˜ ì„ê³„ê°’ íŠœë‹ ì ìš©

    Args:
        market: ì‹œì¥ ì½”ë“œ
        years: í•™ìŠµ ì—°ë„
        threshold: ì´ˆê¸° ì´ë²¤íŠ¸ ê°ì§€ ì„ê³„ê°’ (CVë¡œ ìë™ íŠœë‹ë¨)
        target: ì˜ˆì¸¡ íƒ€ê²Ÿ ("big_move_event", "big_directional_event" ë“±)
        max_tickers: ìµœëŒ€ ì¢…ëª© ìˆ˜
        save_model: ëª¨ë¸ ì €ì¥ ì—¬ë¶€
        sequence_length: TCN ì‹œí€€ìŠ¤ ê¸¸ì´ (L=60)
        batch_size: ë°°ì¹˜ í¬ê¸°
        epochs: ìµœëŒ€ ì—í¬í¬ ìˆ˜
        learning_rate: í•™ìŠµë¥ 
        use_class_balanced_sampler: í´ë˜ìŠ¤ ê· í˜• ìƒ˜í”ŒëŸ¬ ì‚¬ìš©
        apply_calibration: í•™ìŠµ í›„ í™•ë¥  êµì • ë° precision íŠœë‹ ì ìš© ì—¬ë¶€
        daily_top_k: ì¼ë³„ ìµœëŒ€ ì´ë²¤íŠ¸ ì˜ˆì¸¡ ìˆ˜ (precision@k ìš©ë„)
        target_precision: ëª©í‘œ precision ê°’ (0.4 = 40%)

    Returns:
        í•™ìŠµëœ EventDetectorManager ëª¨ë¸
    """
    print(f"[Intraday range ê¸°ë°˜ TCN Event Detector ìƒì„±]")
    print(f"  ì‹œì¥: {market}")
    print(f"  ì—°ë„: {years}")
    print(f"  ì„ê³„ê°’: {threshold:.2f} (ATR% ë°°ìˆ˜)")
    print(f"  íƒ€ê²Ÿ: {target}")
    print(f"  ì‹œí€€ìŠ¤ ê¸¸ì´: {sequence_length}")
    print(f"  ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"  ì—í¬í¬: {epochs}")
    print(f"  í•™ìŠµë¥ : {learning_rate}")
    print(f"  í´ë˜ìŠ¤ ê· í˜• ìƒ˜í”ŒëŸ¬: {use_class_balanced_sampler}")
    print(f"  ğŸ“Š Precision@k ì„¤ì • - Daily top-k: {daily_top_k}, Target precision: {target_precision:.1%}")
    print("  ì´ë²¤íŠ¸ ë¼ë²¨: |ìˆ˜ìµë¥ | >= ì„ê³„ê°’ Ã— ATR%")
    print("=" * 50)

    # ëª¨ë¸ ìƒì„±
    detector = EventDetectorManager(
        threshold=threshold,
        sequence_length=sequence_length
    )

    # Precision@k ì„¤ì •
    detector.daily_top_k = daily_top_k
    detector.target_precision = target_precision

    # Calibration ì ìš© ì„¤ì •
    detector.apply_calibration_after_training = apply_calibration

    # ë°ì´í„° ë¡œë“œ
    df = detector.load_data(
        market=market,
        years=years,
        max_tickers=max_tickers,
        normalize_features=False  # ì›ë³¸ ê°’ ì‚¬ìš©
    )

    # ëª¨ë¸ í•™ìŠµ
    results = detector.train(
        df=df,
        target_col=target,
        batch_size=batch_size,
        epochs=epochs,
        learning_rate=learning_rate,
        use_class_balanced_sampler=use_class_balanced_sampler
    )

    # ëª¨ë¸ ì €ì¥
    if save_model:
        model_name = f"tcn_event_detector_{market}_{'_'.join(map(str, years))}_{int(threshold*100)}pct_L{sequence_length}"
        save_path = f"models/saved/{model_name}"
        detector.save_model(save_path)

    print(f"\n[ATR ìˆ˜ìµë¥  ê¸°ë°˜ TCN Event Detector ì™„ì„±]")
    print(f"  ìµœì¢… ì •í™•ë„: {results['accuracy']:.3f}")
    print(f"  ìµœì¢… F1 ìŠ¤ì½”ì–´: {results['f1_score']:.3f}")
    print(f"  ìµœì¢… ROC-AUC: {results['roc_auc']:.3f}")
    print(f"  ìµœì¢… PR-AUC: {results['pr_auc']:.3f}")
    print(f"  í•™ìŠµ ì—í¬í¬: {results['epochs_trained']}")
    print(f"  ğŸ“Š ì´ë²¤íŠ¸ ë¼ë²¨: |ìˆ˜ìµë¥ | >= {threshold:.2f} Ã— ATR%")

    # Precision@k ê²°ê³¼ ì¶œë ¥
    if 'precision_at_k' in results and results['precision_at_k']:
        print("  ğŸ“Š Precision@k ê²°ê³¼:")
        for k, precision in results['precision_at_k'].items():
            print(f"    {k}: {precision:.3f}")

    # í´ë˜ìŠ¤ ë¶„í¬ ì •ë³´ ì¶œë ¥
    if 'class_distribution' in results:
        cd = results['class_distribution']
        total_samples = cd['total_negative'] + cd['total_positive']
        print(f"  ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬: Negative={cd['total_negative']:,} ({cd['total_negative']/total_samples:.1%}), "
              f"Positive={cd['total_positive']:,} ({cd['total_positive']/total_samples:.1%})")

    return detector


if __name__ == "__main__":
    try:
        print("ğŸš€ Intraday range ê¸°ë°˜ TCN Event Detector")
        print("íŠ¹ì§•:")
        print("  â€¢ TCN ì•„í‚¤í…ì²˜ (L=60, dilation pyramid: 1,2,4,8,16)")
        print("  â€¢ Intraday range ê¸°ë°˜ ì´ë²¤íŠ¸ ë¼ë²¨: max(high/open-1, open/low-1) >= k Ã— ATR%")
        print("  â€¢ ì—°ë„ë³„ CVë¥¼ í†µí•œ ìë™ k íŠœë‹")
        print("  â€¢ í™•ë¥  êµì • (Isotonic Regression)")
        print("  â€¢ Precision@k ê¸°ë°˜ ì„ê³„ê°’ íŠœë‹")
        print("  â€¢ FocalLoss, Class-balanced sampler")
        print("  â€¢ ë³€ë™ì„± ê¸°ë°˜ í”¼ì²˜ ì„¸íŠ¸ ì‚¬ìš©")
        print("=" * 50)

        detector = create_event_detector_model(
            market="KR",
            years=[2018, 2019, 2020],
            threshold=1.0,  # ATR%ì˜ 1ë°° (ìë™ íŠœë‹ë¨)
            target="big_move_event",
            max_tickers=500,
            save_model=True,
            sequence_length=60,
            batch_size=64,
            epochs=50,
            learning_rate=1e-3,
            use_class_balanced_sampler=True,
            apply_calibration=True,
            daily_top_k=5,      # ì¼ë³„ ìµœëŒ€ 5ê°œ ì´ë²¤íŠ¸ ì˜ˆì¸¡
            target_precision=0.4  # ëª©í‘œ precision 40%
        )

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ (í•™ìŠµëœ ëª¨ë¸ê³¼ ë™ì¼í•œ threshold ì‚¬ìš©)
        print("\n[í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€]")
        print(f"  í•™ìŠµëœ threshold: {detector.threshold:.3f}")

        # í•™ìŠµëœ ëª¨ë¸ê³¼ ë™ì¼í•œ thresholdë¡œ í…ŒìŠ¤íŠ¸ detector ìƒì„±
        test_detector = EventDetectorManager(threshold=detector.threshold, sequence_length=60)

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ (í•™ìŠµëœ thresholdë¡œ ë¼ë²¨ ê³„ì‚°)
        test_raw_df = test_detector.load_data(
            market="KR",
            years=[2021],
            max_tickers=20,
            normalize_features=False
        )

        if len(test_raw_df) > 0:
            # í‰ê°€ ì‹œ ì´ë²¤íŠ¸ ë¼ë²¨ ì¬ê³„ì‚° (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ threshold ì‚¬ìš©)
            test_processed_df = test_detector._calculate_event_labels(test_raw_df)
            detector.evaluate(test_processed_df)
            detector.plot_event_distribution(test_processed_df, show=False)

        print("\nâœ… Intraday range ê¸°ë°˜ TCN Event Detector ì™„ë£Œ!")
        print("ğŸ“Š ì´ë²¤íŠ¸ ë¼ë²¨: max(high/open-1, open/low-1) >= ì„ê³„ê°’ Ã— ATR%")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
